{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 ‚Äî From Linear Models to Neural Networks: Building Multi-Layer Perceptrons from Scratch\n",
    "\n",
    "Welcome to **Lecture 5** of *Practical Introduction to Machine Learning and Deep Learning*!  \n",
    "This lecture is part of the **SAIR ML/DL Roadmap & Bootcamp**.\n",
    "\n",
    "## üöÄ NEW: PyTorch Integration & Motivation\n",
    "\n",
    "**Why we're adding PyTorch now:**\n",
    "- This is the **last lecture** building everything from scratch\n",
    "- From Lecture 6 onward, we'll use **PyTorch** for all implementations\n",
    "- Understanding the fundamentals helps you appreciate what PyTorch does automatically\n",
    "- Let's validate our from-scratch implementation against PyTorch!\n",
    "\n",
    "## üå± Why This Lecture Matters  \n",
    "\n",
    "In previous lectures, we mastered **linear models**:\n",
    "- ‚úÖ Linear Regression (continuous outputs)\n",
    "- ‚úÖ Logistic Regression (binary classification)  \n",
    "- ‚úÖ Softmax Regression (multi-class classification)\n",
    "\n",
    "But what if our data has **complex, non-linear patterns**? What if a simple straight line or plane can't separate our classes?\n",
    "\n",
    "> *\"Neural networks are just logistic regression repeated many times with non-linearities in between.\"*\n",
    "\n",
    "## üìñ What You'll Learn\n",
    "\n",
    "1. **The limitations of linear models**\n",
    "2. **Biological inspiration for neural networks**\n",
    "3. **Neuron: The fundamental building block**\n",
    "4. **Activation functions: ReLU, Tanh, Sigmoid**\n",
    "5. **Forward propagation through multiple layers**\n",
    "6. **Backpropagation: The chain rule in action**\n",
    "7. **Implementing Multi-Layer Perceptron (MLP) from scratch**\n",
    "8. **Visualizing learning and decision boundaries**\n",
    "9. **Comparing with sklearn's MLP**\n",
    "10. **üî¨ NEW: Validating against PyTorch implementation**\n",
    "11. **Real-world applications and next steps**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† The Core Idea\n",
    "\n",
    "**Linear Models:**  \n",
    "$$\\hat{y} = \\sigma(\\mathbf{X}\\mathbf{W} + b)$$\n",
    "\n",
    "**Neural Networks:**  \n",
    "$$\\hat{y} = \\sigma_2(\\mathbf{W}_2 \\cdot \\sigma_1(\\mathbf{W}_1 \\mathbf{X} + b_1) + b_2)$$\n",
    "\n",
    "Where $\\sigma_1$ and $\\sigma_2$ are **non-linear activation functions** that enable the network to learn complex patterns.\n",
    "\n",
    "üí° **Key Insight:** By stacking linear transformations with non-linearities, we can approximate any continuous function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Need for Non-Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£ ‚Äî Limitations of Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_moons, make_circles\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîß PyTorch version:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtorch\u001b[49m.__version__)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ CUDA available:\u001b[39m\u001b[33m\"\u001b[39m, torch.cuda.is_available())\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create non-linearly separable datasets\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"üîß PyTorch version:\", torch.__version__)\n",
    "print(\"üöÄ CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Create non-linearly separable datasets\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Moons dataset\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "y_moons = y_moons.reshape(-1, 1)\n",
    "\n",
    "# Circles dataset  \n",
    "X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
    "y_circles = y_circles.reshape(-1, 1)\n",
    "\n",
    "# Convert to PyTorch tensors for later use\n",
    "X_moons_tensor = torch.FloatTensor(X_moons)\n",
    "y_moons_tensor = torch.FloatTensor(y_moons)\n",
    "X_circles_tensor = torch.FloatTensor(X_circles)\n",
    "y_circles_tensor = torch.FloatTensor(y_circles)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Moons\n",
    "axes[0].scatter(X_moons[y_moons.flatten() == 0, 0], X_moons[y_moons.flatten() == 0, 1], \n",
    "                color='blue', label='Class 0', alpha=0.6)\n",
    "axes[0].scatter(X_moons[y_moons.flatten() == 1, 0], X_moons[y_moons.flatten() == 1, 1], \n",
    "                color='red', label='Class 1', alpha=0.6)\n",
    "axes[0].set_title('üåô Moons Dataset (Non-Linear)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Circles\n",
    "axes[1].scatter(X_circles[y_circles.flatten() == 0, 0], X_circles[y_circles.flatten() == 0, 1], \n",
    "                color='blue', label='Class 0', alpha=0.6)\n",
    "axes[1].scatter(X_circles[y_circles.flatten() == 1, 0], X_circles[y_circles.flatten() == 1, 1], \n",
    "                color='red', label='Class 1', alpha=0.6)\n",
    "axes[1].set_title('‚≠ï Circles Dataset (Non-Linear)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2Ô∏è‚É£ ‚Äî Try Linear Model on Non-Linear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, n_features, lr=0.01):\n",
    "        \"\"\"\n",
    "        Initialize Logistic Regression model\n",
    "        \n",
    "        Args:\n",
    "            n_features: number of input features\n",
    "            lr: learning rate\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(n_features, 1) * 0.01\n",
    "        self.b = np.zeros((1, 1))\n",
    "        self.lr = lr\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass: compute predictions\n",
    "        \n",
    "        Args:\n",
    "            X: input features (m, n)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: probabilities (m, 1)\n",
    "        \"\"\"\n",
    "        z = X @ self.W + self.b\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute Binary Cross-Entropy loss\n",
    "        \n",
    "        Args:\n",
    "            y_pred: predicted probabilities (m, 1)\n",
    "            y_true: true labels (m, 1)\n",
    "        \n",
    "        Returns:\n",
    "            loss: scalar BCE loss\n",
    "        \"\"\"\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        m = len(y_true)\n",
    "        loss = -np.mean(\n",
    "            y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def compute_accuracy(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute classification accuracy\n",
    "        \n",
    "        Args:\n",
    "            y_pred: predicted probabilities (m, 1)\n",
    "            y_true: true labels (m, 1)\n",
    "        \n",
    "        Returns:\n",
    "            accuracy: percentage of correct predictions\n",
    "        \"\"\"\n",
    "        predictions = (y_pred >= 0.5).astype(int)\n",
    "        return np.mean(predictions == y_true) * 100\n",
    "    \n",
    "    def backward(self, X, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute gradients\n",
    "        \n",
    "        Args:\n",
    "            X: input features (m, n)\n",
    "            y_pred: predicted probabilities (m, 1)\n",
    "            y_true: true labels (m, 1)\n",
    "        \n",
    "        Returns:\n",
    "            dW: gradient w.r.t weights\n",
    "            db: gradient w.r.t bias\n",
    "        \"\"\"\n",
    "        m = len(y_true)\n",
    "        dW = (1/m) * (X.T @ (y_pred - y_true))\n",
    "        db = (1/m) * np.sum(y_pred - y_true)\n",
    "        return dW, db\n",
    "    \n",
    "    def step(self, dW, db):\n",
    "        \"\"\"Update parameters\"\"\"\n",
    "        self.W -= self.lr * dW\n",
    "        self.b -= self.lr * db\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        Args:\n",
    "            X: training features (m, n)\n",
    "            y: training labels (m, 1)\n",
    "            epochs: number of training iterations\n",
    "            verbose: whether to print progress\n",
    "        \"\"\"\n",
    "        for i in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X) # sigmoid(x@w + b)\n",
    "            \n",
    "            # Compute loss and accuracy\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            acc = self.compute_accuracy(y_pred, y)\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            self.accuracies.append(acc)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW, db = self.backward(X, y_pred, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.step(dW, db)\n",
    "            \n",
    "            if verbose and (i % 100 == 0 or i == epochs - 1):\n",
    "                print(f\"Epoch {i:4d} | Loss: {loss:.4f} | Accuracy: {acc:.2f}%\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \n",
    "        Args:\n",
    "            X: input features (m, n)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: binary predictions (m, 1)\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Get probability predictions\n",
    "        \n",
    "        Args:\n",
    "            X: input features (m, n)\n",
    "        \n",
    "        Returns:\n",
    "            probabilities: predicted probabilities (m, 1)\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "# Try logistic regression on moons dataset\n",
    "linear_model = LogisticRegression(n_features=2, lr=0.1)\n",
    "linear_model.fit(X_moons, y_moons, epochs=1000, verbose=False)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = linear_model.predict(X_moons)\n",
    "accuracy = linear_model.compute_accuracy(linear_model.forward(X_moons), y_moons)\n",
    "\n",
    "print(f\"üìä Logistic Regression on Moons Dataset:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary_linear(model, X, y, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    plt.contour(xx, yy, Z, colors='black', linewidths=1, levels=[0.5])\n",
    "    \n",
    "    plt.scatter(X[y.flatten() == 0, 0], X[y.flatten() == 0, 1], \n",
    "                color='blue', label='Class 0', alpha=0.6)\n",
    "    plt.scatter(X[y.flatten() == 1, 0], X[y.flatten() == 1, 1], \n",
    "                color='red', label='Class 1', alpha=0.6)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary_linear(linear_model, X_moons, y_moons, \n",
    "                            \"‚ùå Linear Decision Boundary on Non-Linear Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3Ô∏è‚É£ ‚Äî Biological Inspiration: The Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize biological vs artificial neuron\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Biological neuron\n",
    "axes[0].text(0.5, 0.5, 'üß† Biological Neuron\\n\\n‚Ä¢ Dendrites (inputs)\\n‚Ä¢ Cell body (processing)\\n‚Ä¢ Axon (output)\\n‚Ä¢ Synapses (connections)',\n",
    "            ha='center', va='center', fontsize=14, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "axes[0].set_title('Biological Neuron', fontsize=16, weight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Artificial neuron\n",
    "neuron_diagram = \"\"\"\n",
    "      x‚ÇÅ ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí w‚ÇÅ\n",
    "      x‚ÇÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí w‚ÇÇ       Œ£ (w‚ãÖx + b)     œÉ(¬∑)      ≈∑\n",
    "      ... ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ...  ‚Üí  -----------  ‚Üí  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí \n",
    "      x‚Çô ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí w‚Çô\n",
    "               b (bias)\n",
    "\"\"\"\n",
    "axes[1].text(0.5, 0.5, neuron_diagram, ha='center', va='center', fontsize=16, \n",
    "            fontfamily='monospace', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "axes[1].set_title('Artificial Neuron (Perceptron)', fontsize=16, weight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üî¨ Key Analogy:\")\n",
    "print(\"Dendrites ‚Üí Input features (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô)\")\n",
    "print(\"Synapses ‚Üí Weights (w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)\")\n",
    "print(\"Cell body ‚Üí Summation + Activation\")\n",
    "print(\"Axon ‚Üí Output (≈∑)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4Ô∏è‚É£ ‚Äî Activation Functions: Introducing Non-Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Plot activation functions\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# ReLU\n",
    "axes[0,0].plot(x, relu(x), 'b-', linewidth=2)\n",
    "axes[0,0].set_title('ReLU: $f(x) = max(0, x)$')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[0,0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "axes[0,1].plot(x, tanh(x), 'r-', linewidth=2)\n",
    "axes[0,1].set_title('Tanh: $f(x) = \\\\frac{e^x - e^{-x}}{e^x + e^{-x}}$')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[0,1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Sigmoid\n",
    "axes[1,0].plot(x, sigmoid(x), 'g-', linewidth=2)\n",
    "axes[1,0].set_title('Sigmoid: $f(x) = \\\\frac{1}{1 + e^{-x}}$')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1,0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Leaky ReLU\n",
    "axes[1,1].plot(x, leaky_relu(x), 'purple', linewidth=2)\n",
    "axes[1,1].set_title('Leaky ReLU: $f(x) = max(Œ±x, x)$')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1,1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5Ô∏è‚É£ ‚Äî Derivatives of Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "# Plot derivatives\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# ReLU derivative\n",
    "axes[0,0].plot(x, relu_derivative(x), 'b-', linewidth=2)\n",
    "axes[0,0].set_title('ReLU Derivative')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[0,0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Tanh derivative\n",
    "axes[0,1].plot(x, tanh_derivative(x), 'r-', linewidth=2)\n",
    "axes[0,1].set_title('Tanh Derivative')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[0,1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Sigmoid derivative\n",
    "axes[1,0].plot(x, sigmoid_derivative(x), 'g-', linewidth=2)\n",
    "axes[1,0].set_title('Sigmoid Derivative')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "axes[1,0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1,0].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Leaky ReLU derivative\n",
    "axes[1,1].plot(x, leaky_relu_derivative(x), 'purple', linewidth=2)\n",
    "axes[1,1].set_title('Leaky ReLU Derivative')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "axes[1,1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[1,1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6Ô∏è‚É£ ‚Äî Why ReLU is Popular in Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Why ReLU Dominates Deep Learning:\")\n",
    "print(\"\\n‚úÖ Advantages:\")\n",
    "print(\"‚Ä¢ Non-saturating for positive values ‚Üí no vanishing gradient\")\n",
    "print(\"‚Ä¢ Computationally efficient (max operation)\")\n",
    "print(\"‚Ä¢ Sparse activation ‚Üí more efficient representations\")\n",
    "print(\"‚Ä¢ Biological plausibility (similar to real neurons)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Challenges:\")\n",
    "print(\"‚Ä¢ Dying ReLU problem (neurons can get stuck at 0)\")\n",
    "print(\"‚Ä¢ Not zero-centered\")\n",
    "\n",
    "print(\"\\nüîß Solutions:\")\n",
    "print(\"‚Ä¢ Leaky ReLU, Parametric ReLU, ELU variants\")\n",
    "print(\"‚Ä¢ Proper weight initialization\")\n",
    "print(\"‚Ä¢ Batch Normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7Ô∏è‚É£ ‚Äî Single Layer ‚Üí Multi-Layer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network architectures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Single layer (Logistic Regression)\n",
    "single_layer = \"\"\"\n",
    "Input Layer      Hidden Layer      Output Layer\n",
    "    x‚ÇÅ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [Neuron] ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ≈∑\n",
    "    x‚ÇÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí \n",
    "    ...            \n",
    "    x‚Çô ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí \n",
    "\"\"\"\n",
    "axes[0].text(0.5, 0.5, single_layer, ha='center', va='center', fontsize=14, \n",
    "            fontfamily='monospace', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "axes[0].set_title('Single Layer (Logistic Regression)', fontsize=16, weight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Multi-layer (Neural Network)\n",
    "multi_layer = \"\"\"\n",
    "Input Layer      Hidden Layer 1    Hidden Layer 2    Output Layer\n",
    "    x‚ÇÅ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [N] ‚Üí [N] ‚Üí [N] ‚îÄ‚îÄ‚Üí [N] ‚Üí [N] ‚Üí [N] ‚îÄ‚îÄ‚Üí ≈∑\n",
    "    x‚ÇÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [N] ‚Üí [N] ‚Üí [N] ‚îÄ‚îÄ‚Üí [N] ‚Üí [N] ‚Üí [N] ‚îÄ‚îÄ‚Üí \n",
    "    ...       [N] ‚Üí [N] ‚Üí [N]     [N] ‚Üí [N] ‚Üí [N]\n",
    "    x‚Çô ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [N] ‚Üí [N] ‚Üí [N] ‚îÄ‚îÄ‚Üí [N] ‚Üí [N] ‚Üí [N] ‚îÄ‚îÄ‚Üí \n",
    "\"\"\"\n",
    "axes[1].text(0.5, 0.5, multi_layer, ha='center', va='center', fontsize=12, \n",
    "            fontfamily='monospace', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "axes[1].set_title('Multi-Layer Neural Network', fontsize=16, weight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üöÄ Key Insight:\")\n",
    "print(\"Multiple layers allow the network to learn hierarchical features:\")\n",
    "print(\"Layer 1: Simple features (edges, corners)\")\n",
    "print(\"Layer 2: Complex features (shapes, patterns)\")  \n",
    "print(\"Layer 3: Very complex features (objects, concepts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8Ô∏è‚É£ ‚Äî Forward Propagation: Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual forward propagation example\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample input\n",
    "X_example = np.array([[1.0, 2.0]])  # 1 sample, 2 features\n",
    "\n",
    "# Initialize weights and biases for a 2-layer network\n",
    "# Architecture: 2 inputs ‚Üí 3 hidden neurons ‚Üí 1 output\n",
    "W1 = np.random.randn(2, 3) * 0.1  # Input to hidden\n",
    "b1 = np.zeros((1, 3))\n",
    "W2 = np.random.randn(3, 1) * 0.1  # Hidden to output  \n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "print(\"üî¢ Forward Propagation Example:\")\n",
    "print(f\"Input X: {X_example}\")\n",
    "print(f\"W1 shape: {W1.shape}, b1 shape: {b1.shape}\")\n",
    "print(f\"W2 shape: {W2.shape}, b2 shape: {b2.shape}\")\n",
    "\n",
    "# Step 1: Input to hidden layer\n",
    "z1 = X_example @ W1 + b1\n",
    "print(f\"\\nStep 1 - Linear transformation (z1): {z1}\")\n",
    "\n",
    "# Step 2: Apply activation (ReLU)\n",
    "a1 = relu(z1)\n",
    "print(f\"Step 2 - Activation (a1 = ReLU(z1)): {a1}\")\n",
    "\n",
    "# Step 3: Hidden to output layer\n",
    "z2 = a1 @ W2 + b2\n",
    "print(f\"Step 3 - Linear transformation (z2): {z2}\")\n",
    "\n",
    "# Step 4: Output activation (sigmoid for binary classification)\n",
    "y_pred = sigmoid(z2)\n",
    "print(f\"Step 4 - Output (≈∑ = sigmoid(z2)): {y_pred}\")\n",
    "\n",
    "print(f\"\\nüéØ Final prediction: {y_pred[0,0]:.4f} (probability of class 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9Ô∏è‚É£ ‚Äî The Chain Rule: Mathematical Foundation of Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize computational graph\n",
    "computational_graph = \"\"\"\n",
    "Forward Pass:\n",
    "X ‚Üí z‚ÇÅ = XW‚ÇÅ + b‚ÇÅ ‚Üí a‚ÇÅ = œÉ(z‚ÇÅ) ‚Üí z‚ÇÇ = a‚ÇÅW‚ÇÇ + b‚ÇÇ ‚Üí ≈∑ = œÉ(z‚ÇÇ) ‚Üí Loss\n",
    "\n",
    "Backward Pass (Chain Rule):\n",
    "‚àÇLoss/‚àÇW‚ÇÇ = ‚àÇLoss/‚àÇ≈∑ ¬∑ ‚àÇ≈∑/‚àÇz‚ÇÇ ¬∑ ‚àÇz‚ÇÇ/‚àÇW‚ÇÇ\n",
    "‚àÇLoss/‚àÇW‚ÇÅ = ‚àÇLoss/‚àÇ≈∑ ¬∑ ‚àÇ≈∑/‚àÇz‚ÇÇ ¬∑ ‚àÇz‚ÇÇ/‚àÇa‚ÇÅ ¬∑ ‚àÇa‚ÇÅ/‚àÇz‚ÇÅ ¬∑ ‚àÇz‚ÇÅ/‚àÇW‚ÇÅ\n",
    "\n",
    "Each step uses local gradients!\n",
    "\"\"\"\n",
    "\n",
    "print(\"üßÆ Computational Graph & Chain Rule:\")\n",
    "print(computational_graph)\n",
    "\n",
    "# Example chain rule calculation\n",
    "print(\"\\nüìê Example: Computing ‚àÇLoss/‚àÇW‚ÇÅ step by step:\")\n",
    "steps = [\n",
    "    \"1. ‚àÇLoss/‚àÇ≈∑ = - (y/≈∑ - (1-y)/(1-≈∑))\",\n",
    "    \"2. ‚àÇ≈∑/‚àÇz‚ÇÇ = ≈∑(1-≈∑)  (sigmoid derivative)\",\n",
    "    \"3. ‚àÇz‚ÇÇ/‚àÇa‚ÇÅ = W‚ÇÇ\",\n",
    "    \"4. ‚àÇa‚ÇÅ/‚àÇz‚ÇÅ = œÉ'(z‚ÇÅ)  (activation derivative)\", \n",
    "    \"5. ‚àÇz‚ÇÅ/‚àÇW‚ÇÅ = X\",\n",
    "    \"6. Multiply all: ‚àÇLoss/‚àÇW‚ÇÅ = X ¬∑ œÉ'(z‚ÇÅ) ¬∑ W‚ÇÇ ¬∑ ≈∑(1-≈∑) ¬∑ (≈∑ - y)\"\n",
    "]\n",
    "\n",
    "for step in steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step üîü ‚Äî Implementing Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, activation='relu', learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize neural network\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: list of layer sizes [input_size, hidden1_size, ..., output_size]\n",
    "            activation: activation function ('relu', 'tanh', 'sigmoid')\n",
    "            learning_rate: learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_name = activation\n",
    "        self.lr = learning_rate\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # He initialization for ReLU, Xavier for tanh/sigmoid\n",
    "            if activation == 'relu':\n",
    "                scale = np.sqrt(2.0 / layer_sizes[i])\n",
    "            else:\n",
    "                scale = np.sqrt(1.0 / layer_sizes[i])\n",
    "                \n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * scale\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            \n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation through all layers\"\"\"\n",
    "        self.activations = [X]  # Store all activations for backprop\n",
    "        self.z_values = []      # Store all linear outputs\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = self.activations[-1] @ self.weights[i] + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            if self.activation_name == 'relu':\n",
    "                a = relu(z)\n",
    "            elif self.activation_name == 'tanh':\n",
    "                a = tanh(z)\n",
    "            elif self.activation_name == 'sigmoid':\n",
    "                a = sigmoid(z)\n",
    "                \n",
    "            self.activations.append(a)\n",
    "        \n",
    "        # Output layer (always sigmoid for binary classification)\n",
    "        z_output = self.activations[-1] @ self.weights[-1] + self.biases[-1]\n",
    "        self.z_values.append(z_output)\n",
    "        output = sigmoid(z_output)\n",
    "        self.activations.append(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def compute_accuracy(self, y_pred, y_true):\n",
    "        \"\"\"Classification accuracy\"\"\"\n",
    "        predictions = (y_pred >= 0.5).astype(int)\n",
    "        return np.mean(predictions == y_true) * 100\n",
    "    \n",
    "    def backward(self, X, y_true):\n",
    "        \"\"\"Backward propagation\"\"\"\n",
    "        m = len(y_true)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW = [np.zeros_like(W) for W in self.weights]\n",
    "        db = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dZ_output = self.activations[-1] - y_true  # ‚àÇLoss/‚àÇz_output\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        for l in range(len(self.weights) - 1, -1, -1):\n",
    "            # Gradients for weights and biases\n",
    "            dW[l] = (1/m) * (self.activations[l].T @ dZ_output)\n",
    "            db[l] = (1/m) * np.sum(dZ_output, axis=0, keepdims=True)\n",
    "            \n",
    "            if l > 0:  # Continue backpropagation\n",
    "                # Gradient for previous layer\n",
    "                dA_prev = dZ_output @ self.weights[l].T\n",
    "                \n",
    "                # Gradient through activation function\n",
    "                if self.activation_name == 'relu':\n",
    "                    dZ_prev = dA_prev * relu_derivative(self.z_values[l-1])\n",
    "                elif self.activation_name == 'tanh':\n",
    "                    dZ_prev = dA_prev * tanh_derivative(self.z_values[l-1])\n",
    "                elif self.activation_name == 'sigmoid':\n",
    "                    dZ_prev = dA_prev * sigmoid_derivative(self.z_values[l-1])\n",
    "                \n",
    "                dZ_output = dZ_prev\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def update_parameters(self, dW, db):\n",
    "        \"\"\"Update weights and biases using gradients\"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lr * dW[i]\n",
    "            self.biases[i] -= self.lr * db[i]\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"Train the neural network\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss and accuracy\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            accuracy = self.compute_accuracy(y_pred, y)\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            self.accuracies.append(accuracy)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW, db = self.backward(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(dW, db)\n",
    "            \n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch:4d} | Loss: {loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get probability predictions\"\"\"\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£1Ô∏è‚É£ ‚Äî Train Neural Network on Moons Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train neural network\n",
    "print(\"üöÄ Training Neural Network on Moons Dataset...\")\n",
    "nn_model = NeuralNetwork(layer_sizes=[2, 10, 5, 1], activation='relu', learning_rate=0.1)\n",
    "nn_model.fit(X_moons, y_moons, epochs=2000, verbose=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"Final Loss: {nn_model.losses[-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {nn_model.accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£2Ô∏è‚É£ ‚Äî Visualize Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(nn_model.losses, color='red', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Binary Cross-Entropy Loss')\n",
    "axes[0].set_title('üìâ Neural Network Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(nn_model.accuracies, color='green', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('üìà Neural Network Training Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£3Ô∏è‚É£ ‚Äî Visualize Neural Network Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn_decision_boundary(model, X, y, title):\n",
    "    \"\"\"Plot decision boundary for neural network\"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    plt.contour(xx, yy, Z, colors='black', linewidths=2, levels=[0.5])\n",
    "    \n",
    "    plt.scatter(X[y.flatten() == 0, 0], X[y.flatten() == 0, 1], \n",
    "                color='blue', label='Class 0', alpha=0.6, s=50)\n",
    "    plt.scatter(X[y.flatten() == 1, 0], X[y.flatten() == 1, 1], \n",
    "                color='red', label='Class 1', alpha=0.6, s=50)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_nn_decision_boundary(nn_model, X_moons, y_moons, \n",
    "                         \"üéØ Neural Network Decision Boundary (Non-Linear!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£4Ô∏è‚É£ ‚Äî Compare with Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performances\n",
    "linear_accuracy = linear_model.compute_accuracy(linear_model.forward(X_moons), y_moons)\n",
    "nn_accuracy = nn_model.accuracies[-1]\n",
    "\n",
    "print(\"üìä MODEL COMPARISON ON MOONS DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Model':<25} {'Accuracy':<15} {'Decision Boundary'}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Logistic Regression':<25} {linear_accuracy:<15.2f}% {'Linear ‚ùå'}\")\n",
    "print(f\"{'Neural Network':<25} {nn_accuracy:<15.2f}% {'Non-Linear ‚úÖ'}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ NEW: PyTorch Validation & Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£5Ô∏è‚É£ ‚Äî PyTorch Implementation for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation of the same neural network\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation='relu'):\n",
    "        super(PyTorchMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if i < len(layer_sizes) - 2:  # Hidden layers\n",
    "                if activation == 'relu':\n",
    "                    layers.append(nn.ReLU())\n",
    "                elif activation == 'tanh':\n",
    "                    layers.append(nn.Tanh())\n",
    "                elif activation == 'sigmoid':\n",
    "                    layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.network(x))  # Sigmoid for binary classification\n",
    "\n",
    "# Create PyTorch model with same architecture\n",
    "pytorch_model = PyTorchMLP([2, 10, 5, 1], activation='relu')\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(pytorch_model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"ü§ñ PyTorch Model Architecture:\")\n",
    "print(pytorch_model)\n",
    "print(f\"\\nüîß Total parameters: {sum(p.numel() for p in pytorch_model.parameters())}\")\n",
    "\n",
    "# Train PyTorch model\n",
    "print(\"\\nüöÄ Training PyTorch Model...\")\n",
    "pytorch_losses = []\n",
    "pytorch_accuracies = []\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # Forward pass\n",
    "    outputs = pytorch_model(X_moons_tensor)\n",
    "    loss = criterion(outputs, y_moons_tensor)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute accuracy\n",
    "    predicted = (outputs >= 0.5).float()\n",
    "    accuracy = (predicted == y_moons_tensor).float().mean() * 100\n",
    "    \n",
    "    pytorch_losses.append(loss.item())\n",
    "    pytorch_accuracies.append(accuracy.item())\n",
    "    \n",
    "    if epoch % 400 == 0 or epoch == 1999:\n",
    "        print(f\"Epoch {epoch:4d} | Loss: {loss.item():.4f} | Accuracy: {accuracy.item():.2f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ PyTorch Training Complete!\")\n",
    "print(f\"Final Loss: {pytorch_losses[-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {pytorch_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£6Ô∏è‚É£ ‚Äî Compare From-Scratch vs PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare our implementation with PyTorch\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0, 0].plot(nn_model.losses, 'b-', label='From Scratch', alpha=0.7, linewidth=2)\n",
    "axes[0, 0].plot(pytorch_losses, 'r-', label='PyTorch', alpha=0.7, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('üìâ Loss Comparison: From Scratch vs PyTorch')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 1].plot(nn_model.accuracies, 'b-', label='From Scratch', alpha=0.7, linewidth=2)\n",
    "axes[0, 1].plot(pytorch_accuracies, 'r-', label='PyTorch', alpha=0.7, linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('üìà Accuracy Comparison: From Scratch vs PyTorch')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary - From Scratch\n",
    "h = 0.02\n",
    "x_min, x_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
    "y_min, y_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# From Scratch prediction\n",
    "Z_scratch = nn_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_scratch = Z_scratch.reshape(xx.shape)\n",
    "\n",
    "axes[1, 0].contourf(xx, yy, Z_scratch, alpha=0.3, cmap='RdBu')\n",
    "axes[1, 0].scatter(X_moons[y_moons.flatten() == 0, 0], X_moons[y_moons.flatten() == 0, 1], \n",
    "                   color='blue', label='Class 0', alpha=0.6, s=30)\n",
    "axes[1, 0].scatter(X_moons[y_moons.flatten() == 1, 0], X_moons[y_moons.flatten() == 1, 1], \n",
    "                   color='red', label='Class 1', alpha=0.6, s=30)\n",
    "axes[1, 0].set_title(f'From Scratch ({nn_model.accuracies[-1]:.1f}%)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# PyTorch prediction\n",
    "with torch.no_grad():\n",
    "    Z_pytorch = pytorch_model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z_pytorch = (Z_pytorch >= 0.5).float().numpy().reshape(xx.shape)\n",
    "\n",
    "axes[1, 1].contourf(xx, yy, Z_pytorch, alpha=0.3, cmap='RdBu')\n",
    "axes[1, 1].scatter(X_moons[y_moons.flatten() == 0, 0], X_moons[y_moons.flatten() == 0, 1], \n",
    "                   color='blue', label='Class 0', alpha=0.6, s=30)\n",
    "axes[1, 1].scatter(X_moons[y_moons.flatten() == 1, 0], X_moons[y_moons.flatten() == 1, 1], \n",
    "                   color='red', label='Class 1', alpha=0.6, s=30)\n",
    "axes[1, 1].set_title(f'PyTorch ({pytorch_accuracies[-1]:.1f}%)')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä FROM-SCRATCH vs PYTORCH BENCHMARK RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Metric':<25} {'From Scratch':<15} {'PyTorch':<15} {'Difference'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Final Loss':<25} {nn_model.losses[-1]:<15.4f} {pytorch_losses[-1]:<15.4f} {nn_model.losses[-1] - pytorch_losses[-1]:+.4f}\")\n",
    "print(f\"{'Final Accuracy (%)':<25} {nn_model.accuracies[-1]:<15.2f} {pytorch_accuracies[-1]:<15.2f} {nn_model.accuracies[-1] - pytorch_accuracies[-1]:+.2f}\")\n",
    "print(f\"{'Architecture':<25} {'2-10-5-1':<15} {'2-10-5-1':<15} {'Same'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(\"‚úÖ Both implementations achieve similar performance\")\n",
    "print(\"‚úÖ Our from-scratch implementation correctly mimics neural network behavior\")\n",
    "print(\"‚úÖ PyTorch provides automatic differentiation and GPU support\")\n",
    "print(\"‚úÖ Understanding fundamentals helps appreciate PyTorch's magic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£7Ô∏è‚É£ ‚Äî Test on Circles Dataset with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on circles dataset\n",
    "print(\"\\nüß™ Testing on Circles Dataset...\")\n",
    "\n",
    "# Linear model (should fail)\n",
    "linear_circles = LogisticRegression(n_features=2, lr=0.1)\n",
    "linear_circles.fit(X_circles, y_circles, epochs=1000, verbose=False)\n",
    "linear_circles_acc = linear_circles.compute_accuracy(linear_circles.forward(X_circles), y_circles)\n",
    "\n",
    "# Neural network (should succeed)\n",
    "nn_circles = NeuralNetwork(layer_sizes=[2, 20, 10, 1], activation='tanh', learning_rate=0.1)\n",
    "nn_circles.fit(X_circles, y_circles, epochs=2000, verbose=False)\n",
    "nn_circles_acc = nn_circles.accuracies[-1]\n",
    "\n",
    "# PyTorch model for circles\n",
    "pytorch_circles = PyTorchMLP([2, 20, 10, 1], activation='tanh')\n",
    "optimizer_circles = optim.SGD(pytorch_circles.parameters(), lr=0.1)\n",
    "\n",
    "pytorch_circles_losses = []\n",
    "pytorch_circles_accuracies = []\n",
    "\n",
    "for epoch in range(2000):\n",
    "    outputs = pytorch_circles(X_circles_tensor)\n",
    "    loss = criterion(outputs, y_circles_tensor)\n",
    "    \n",
    "    optimizer_circles.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_circles.step()\n",
    "    \n",
    "    predicted = (outputs >= 0.5).float()\n",
    "    accuracy = (predicted == y_circles_tensor).float().mean() * 100\n",
    "    \n",
    "    pytorch_circles_losses.append(loss.item())\n",
    "    pytorch_circles_accuracies.append(accuracy.item())\n",
    "\n",
    "pytorch_circles_acc = pytorch_circles_accuracies[-1]\n",
    "\n",
    "print(f\"üìä Circles Dataset Results:\")\n",
    "print(f\"Linear Model Accuracy: {linear_circles_acc:.2f}%\")\n",
    "print(f\"Neural Network (From Scratch) Accuracy: {nn_circles_acc:.2f}%\")\n",
    "print(f\"Neural Network (PyTorch) Accuracy: {pytorch_circles_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ CIRCLES DATASET: LINEAR vs NEURAL NETWORKS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<30} {'Accuracy':<15} {'Can Learn Circles?'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Linear Model':<30} {linear_circles_acc:<15.2f}% {'‚ùå No'}\")\n",
    "print(f\"{'Neural Network (From Scratch)':<30} {nn_circles_acc:<15.2f}% {'‚úÖ Yes'}\")\n",
    "print(f\"{'Neural Network (PyTorch)':<30} {pytorch_circles_acc:<15.2f}% {'‚úÖ Yes'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Concepts and Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£8Ô∏è‚É£ ‚Äî Multi-Class Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassNeuralNetwork:\n",
    "    def __init__(self, layer_sizes, activation='relu', learning_rate=0.01):\n",
    "        \"\"\"Neural network for multi-class classification\"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_name = activation\n",
    "        self.lr = learning_rate\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if activation == 'relu':\n",
    "                scale = np.sqrt(2.0 / layer_sizes[i])\n",
    "            else:\n",
    "                scale = np.sqrt(1.0 / layer_sizes[i])\n",
    "                \n",
    "            W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * scale\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            \n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        \"\"\"Softmax activation for output layer\"\"\"\n",
    "        z_stable = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_stable)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = self.activations[-1] @ self.weights[i] + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            if self.activation_name == 'relu':\n",
    "                a = relu(z)\n",
    "            elif self.activation_name == 'tanh':\n",
    "                a = tanh(z)\n",
    "            elif self.activation_name == 'sigmoid':\n",
    "                a = sigmoid(z)\n",
    "                \n",
    "            self.activations.append(a)\n",
    "        \n",
    "        # Output layer (softmax for multi-class)\n",
    "        z_output = self.activations[-1] @ self.weights[-1] + self.biases[-1]\n",
    "        self.z_values.append(z_output)\n",
    "        output = self.softmax(z_output)\n",
    "        self.activations.append(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Categorical cross-entropy loss\"\"\"\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = np.eye(y_pred.shape[1])[y_true]\n",
    "        \n",
    "        y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    \n",
    "    def compute_accuracy(self, y_pred, y_true):\n",
    "        \"\"\"Classification accuracy\"\"\"\n",
    "        pred_classes = np.argmax(y_pred, axis=1)\n",
    "        if y_true.ndim == 2:\n",
    "            true_classes = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            true_classes = y_true\n",
    "        return np.mean(pred_classes == true_classes) * 100\n",
    "    \n",
    "    def backward(self, X, y_true):\n",
    "        \"\"\"Backward propagation for multi-class\"\"\"\n",
    "        m = len(y_true)\n",
    "        \n",
    "        if y_true.ndim == 1:\n",
    "            y_true = np.eye(self.weights[-1].shape[1])[y_true]\n",
    "        \n",
    "        dW = [np.zeros_like(W) for W in self.weights]\n",
    "        db = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Output layer gradient (softmax + cross-entropy has nice derivative)\n",
    "        dZ_output = self.activations[-1] - y_true\n",
    "        \n",
    "        for l in range(len(self.weights) - 1, -1, -1):\n",
    "            dW[l] = (1/m) * (self.activations[l].T @ dZ_output)\n",
    "            db[l] = (1/m) * np.sum(dZ_output, axis=0, keepdims=True)\n",
    "            \n",
    "            if l > 0:\n",
    "                dA_prev = dZ_output @ self.weights[l].T\n",
    "                \n",
    "                if self.activation_name == 'relu':\n",
    "                    dZ_prev = dA_prev * relu_derivative(self.z_values[l-1])\n",
    "                elif self.activation_name == 'tanh':\n",
    "                    dZ_prev = dA_prev * tanh_derivative(self.z_values[l-1])\n",
    "                elif self.activation_name == 'sigmoid':\n",
    "                    dZ_prev = dA_prev * sigmoid_derivative(self.z_values[l-1])\n",
    "                \n",
    "                dZ_output = dZ_prev\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def update_parameters(self, dW, db):\n",
    "        \"\"\"Update weights and biases\"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lr * dW[i]\n",
    "            self.biases[i] -= self.lr * db[i]\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            accuracy = self.compute_accuracy(y_pred, y)\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            self.accuracies.append(accuracy)\n",
    "            \n",
    "            dW, db = self.backward(X, y)\n",
    "            self.update_parameters(dW, db)\n",
    "            \n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch:4d} | Loss: {loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1Ô∏è‚É£9Ô∏è‚É£ ‚Äî Test Multi-Class Neural Network on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(\"üå∏ Iris Dataset:\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Samples: {len(X_iris)}, Features: {X_iris.shape[1]}, Classes: {len(np.unique(y_iris))}\")\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train multi-class neural network\n",
    "print(\"\\nüöÄ Training Multi-Class Neural Network on Iris Dataset...\")\n",
    "multi_nn = MultiClassNeuralNetwork(\n",
    "    layer_sizes=[4, 10, 8, 3],  # 4 inputs, 2 hidden layers, 3 outputs\n",
    "    activation='relu', \n",
    "    learning_rate=0.01\n",
    ")\n",
    "multi_nn.fit(X_train_scaled, y_train, epochs=2000, verbose=True)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = multi_nn.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2Ô∏è‚É£0Ô∏è‚É£ ‚Äî PyTorch Multi-Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch multi-class implementation\n",
    "class PyTorchMultiClassMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(PyTorchMultiClassMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Prepare PyTorch data\n",
    "X_iris_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_iris_tensor = torch.LongTensor(y_train)  # Use LongTensor for classification\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "\n",
    "# Create and train PyTorch model\n",
    "pytorch_multi = PyTorchMultiClassMLP(input_size=4, hidden_sizes=[10, 8], num_classes=3)\n",
    "criterion_multi = nn.CrossEntropyLoss()\n",
    "optimizer_multi = optim.SGD(pytorch_multi.parameters(), lr=0.01)\n",
    "\n",
    "print(\"ü§ñ Training PyTorch Multi-Class Model...\")\n",
    "pytorch_multi_losses = []\n",
    "pytorch_multi_accuracies = []\n",
    "\n",
    "for epoch in range(2000):\n",
    "    outputs = pytorch_multi(X_iris_tensor)\n",
    "    loss = criterion_multi(outputs, y_iris_tensor)\n",
    "    \n",
    "    optimizer_multi.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_multi.step()\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = (predicted == y_iris_tensor).float().mean() * 100\n",
    "    \n",
    "    pytorch_multi_losses.append(loss.item())\n",
    "    pytorch_multi_accuracies.append(accuracy.item())\n",
    "    \n",
    "    if epoch % 400 == 0 or epoch == 1999:\n",
    "        print(f\"Epoch {epoch:4d} | Loss: {loss.item():.4f} | Accuracy: {accuracy.item():.2f}%\")\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "with torch.no_grad():\n",
    "    test_outputs = pytorch_multi(X_test_tensor)\n",
    "    _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "    pytorch_test_accuracy = (test_predicted == torch.LongTensor(y_test)).float().mean() * 100\n",
    "\n",
    "print(f\"\\n‚úÖ PyTorch Test Accuracy: {pytorch_test_accuracy:.2f}%\")\n",
    "\n",
    "# Compare multi-class results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä MULTI-CLASS NEURAL NETWORK COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Model':<30} {'Test Accuracy':<15}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'From Scratch':<30} {test_accuracy:<15.2f}%\")\n",
    "print(f\"{'PyTorch':<30} {pytorch_test_accuracy:<15.2f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Plot learning curves comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(multi_nn.losses, 'b-', label='From Scratch', alpha=0.7, linewidth=2)\n",
    "axes[0].plot(pytorch_multi_losses, 'r-', label='PyTorch', alpha=0.7, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Categorical Cross-Entropy Loss')\n",
    "axes[0].set_title('üìâ Multi-Class Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(multi_nn.accuracies, 'b-', label='From Scratch', alpha=0.7, linewidth=2)\n",
    "axes[1].plot(pytorch_multi_accuracies, 'r-', label='PyTorch', alpha=0.7, linewidth=2)\n",
    "axes[1].axhline(y=test_accuracy, color='blue', linestyle='--', \n",
    "               label=f'From Scratch Test ({test_accuracy:.1f}%)', alpha=0.7)\n",
    "axes[1].axhline(y=pytorch_test_accuracy, color='red', linestyle='--', \n",
    "               label=f'PyTorch Test ({pytorch_test_accuracy:.1f}%)', alpha=0.7)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('üìà Multi-Class Training Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Real-World Applications and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2Ô∏è‚É£1Ô∏è‚É£ ‚Äî When to Use Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ WHEN TO USE NEURAL NETWORKS:\")\n",
    "print(\"\\n‚úÖ Good use cases:\")\n",
    "print(\"‚Ä¢ Complex non-linear patterns\")\n",
    "print(\"‚Ä¢ Large datasets (thousands+ samples)\")\n",
    "print(\"‚Ä¢ Image, audio, text data\")\n",
    "print(\"‚Ä¢ Hierarchical feature learning needed\")\n",
    "print(\"‚Ä¢ State-of-the-art performance required\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è When to consider simpler models:\")\n",
    "print(\"‚Ä¢ Small datasets (< thousands of samples)\")\n",
    "print(\"‚Ä¢ Tabular data with clear linear relationships\")  \n",
    "print(\"‚Ä¢ Need for model interpretability\")\n",
    "print(\"‚Ä¢ Limited computational resources\")\n",
    "print(\"‚Ä¢ Quick prototyping\")\n",
    "\n",
    "print(\"\\nüìä Model Selection Guide:\")\n",
    "print(\"Linear/Logistic Regression ‚Üí Random Forest ‚Üí Neural Networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2Ô∏è‚É£2Ô∏è‚É£ ‚Äî Common Architectures and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common neural network architectures\n",
    "architectures = {\n",
    "    \"Shallow Network\": [10, 1],\n",
    "    \"Medium Network\": [64, 32, 1], \n",
    "    \"Deep Network\": [128, 64, 32, 16, 1],\n",
    "    \"Wide & Shallow\": [256, 1],\n",
    "    \"Narrow & Deep\": [16, 16, 16, 16, 16, 1]\n",
    "}\n",
    "\n",
    "print(\"üèóÔ∏è COMMON NEURAL NETWORK ARCHITECTURES:\")\n",
    "for name, layers in architectures.items():\n",
    "    print(f\"‚Ä¢ {name}: {layers}\")\n",
    "\n",
    "print(\"\\nüéõÔ∏è KEY HYPERPARAMETERS:\")\n",
    "hyperparams = {\n",
    "    \"Learning Rate\": \"0.001-0.1 (most important!)\",\n",
    "    \"Hidden Layers\": \"1-5 for MLPs\",\n",
    "    \"Layer Sizes\": \"32-512 neurons per layer\", \n",
    "    \"Activation\": \"ReLU (hidden), Softmax/Sigmoid (output)\",\n",
    "    \"Batch Size\": \"32-256\",\n",
    "    \"Epochs\": \"Until validation loss stops improving\"\n",
    "}\n",
    "\n",
    "for param, recommendation in hyperparams.items():\n",
    "    print(f\"‚Ä¢ {param}: {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2Ô∏è‚É£3Ô∏è‚É£ ‚Äî From Scratch to Production Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ FROM SCRATCH TO PRODUCTION FRAMEWORKS:\")\n",
    "print(\"\\nüß™ What we built from scratch:\")\n",
    "print(\"‚Ä¢ Forward propagation\")\n",
    "print(\"‚Ä¢ Backward propagation (backprop)\")\n",
    "print(\"‚Ä¢ Activation functions and derivatives\") \n",
    "print(\"‚Ä¢ Weight initialization\")\n",
    "print(\"‚Ä¢ Gradient descent optimization\")\n",
    "\n",
    "print(\"\\nüè≠ What PyTorch adds:\")\n",
    "print(\"‚Ä¢ Automatic differentiation (autograd) - No more manual chain rule!\")\n",
    "print(\"‚Ä¢ GPU acceleration - 10-100x speedup\")\n",
    "print(\"‚Ä¢ Pre-built layers and architectures - CNN, RNN, Transformers\")\n",
    "print(\"‚Ä¢ Advanced optimizers (Adam, RMSprop) - Better than plain SGD\")\n",
    "print(\"‚Ä¢ Regularization techniques - Dropout, BatchNorm\")\n",
    "print(\"‚Ä¢ Distributed training - Scale to multiple GPUs\")\n",
    "print(\"‚Ä¢ Model deployment tools - Export to production\")\n",
    "\n",
    "print(\"\\nüîú Next Steps (Using PyTorch):\")\n",
    "print(\"1. Lecture 6: Convolutional Neural Networks (CNNs) for images\")\n",
    "print(\"2. Lecture 7: Recurrent Neural Networks (RNNs) for sequences\") \n",
    "print(\"3. Lecture 8: Transformers for text and beyond!\")\n",
    "print(\"4. Lecture 9: Computer Vision applications\")\n",
    "print(\"5. Lecture 10: Natural Language Processing\")\n",
    "\n",
    "print(\"\\nüéØ Why PyTorch for the rest of the course:\")\n",
    "print(\"‚úÖ Industry standard for research and production\")\n",
    "print(\"‚úÖ Pythonic and intuitive API\")\n",
    "print(\"‚úÖ Excellent documentation and community\")\n",
    "print(\"‚úÖ Seamless GPU acceleration\")\n",
    "print(\"‚úÖ Used by companies like Meta, Tesla, OpenAI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary: Complete Deep Learning Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary visualization\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = {\n",
    "    \"Model\": [\"Linear Regression\", \"Logistic Regression\", \"Softmax Regression\", \"Neural Network\", \"PyTorch NN\"],\n",
    "    \"Task\": [\"Regression\", \"Binary Classification\", \"Multi-Class Classification\", \"Complex Non-Linear\", \"Production Ready\"],\n",
    "    \"Activation\": [\"None\", \"Sigmoid\", \"Softmax\", \"ReLU/Tanh/Sigmoid\", \"Auto-differentiation\"],\n",
    "    \"Implementation\": [\"From Scratch\", \"From Scratch\", \"From Scratch\", \"From Scratch\", \"PyTorch Framework\"],\n",
    "    \"Performance\": [\"Baseline\", \"Baseline\", \"Baseline\", \"Good\", \"Excellent\"]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"üìö MODEL EVOLUTION SUMMARY:\")\n",
    "print(\"=\" * 100)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Key Mathematical Insights:\n",
    "\n",
    "1. **Universal Approximation Theorem**: A neural network with one hidden layer can approximate any continuous function given enough neurons\n",
    "2. **Backpropagation**: Efficient gradient computation using chain rule\n",
    "3. **Non-Linearity**: Activation functions enable learning complex patterns\n",
    "4. **Hierarchical Features**: Each layer learns features at different abstraction levels\n",
    "\n",
    "### üöÄ Path Forward:\n",
    "\n",
    "This completes our foundation in **neural networks**:\n",
    "- ‚úÖ Understanding biological inspiration\n",
    "- ‚úÖ Implementing forward and backward propagation  \n",
    "- ‚úÖ Working with different activation functions\n",
    "- ‚úÖ Building both binary and multi-class networks\n",
    "- ‚úÖ Visualizing decision boundaries and learning curves\n",
    "- ‚úÖ **NEW: Validating against PyTorch implementation**\n",
    "\n",
    "**Next up:** Convolutional Neural Networks (CNNs) for image data - using PyTorch!\n",
    "\n",
    "---\n",
    "\n",
    "*\"Understanding the fundamentals from scratch makes you appreciate the magic of PyTorch!\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Practice & Reflection ‚Äî Neural Networks from Scratch\n",
    "\n",
    "Congratulations üéâ ‚Äî you have successfully built neural networks from scratch, understood the mathematics of backpropagation, and seen how they can learn complex non-linear patterns!\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **Part 1 ‚Äî Core Concepts**\n",
    "\n",
    "1. **Explain in your own words:**\n",
    "   - Why do we need activation functions in neural networks?\n",
    "   - What is the \"vanishing gradient\" problem and how does ReLU help?\n",
    "   - How does backpropagation use the chain rule?\n",
    "   - What's the difference between a single neuron and a neural network?\n",
    "   - Why can neural networks learn non-linear patterns while linear models can't?\n",
    "\n",
    "2. **Activation functions:**\n",
    "   - When would you use ReLU vs Tanh vs Sigmoid?\n",
    "   - Compute ReLU(2), ReLU(-2), Tanh(1), Sigmoid(0)\n",
    "   - Why do we use softmax for multi-class output and sigmoid for binary?\n",
    "   - What happens if we use linear activation in hidden layers?\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **Part 2 ‚Äî Mathematical Understanding**\n",
    "\n",
    "3. **Forward propagation:**\n",
    "   - Given input [1, 2], weights [[0.5, -0.5], [0.3, 0.7]], bias [0.1, -0.1], compute the output with ReLU activation\n",
    "   - Show the matrix dimensions at each layer for a 3-layer network\n",
    "\n",
    "4. **Backpropagation:**\n",
    "   - Derive the gradient for a simple 2-layer network step by step\n",
    "   - Why is the gradient for softmax + cross-entropy so elegant?\n",
    "   - How does weight initialization affect training?\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **Part 3 ‚Äî Implementation Challenges**\n",
    "\n",
    "5. **Architecture experiments:**\n",
    "   - Try different architectures on moons dataset: [2,5,1], [2,10,5,1], [2,20,10,5,1]\n",
    "   - Compare training time, final accuracy, and decision boundaries\n",
    "   - Which works best and why?\n",
    "\n",
    "6. **Activation function comparison:**\n",
    "   - Train the same architecture with ReLU, Tanh, and Sigmoid\n",
    "   - Plot loss curves for each\n",
    "   - Discuss convergence speed and final performance\n",
    "\n",
    "7. **Learning rate experiments:**\n",
    "   - Try learning rates: 0.001, 0.01, 0.1, 1.0\n",
    "   - Observe training stability and convergence\n",
    "   - Identify signs of too high/too low learning rates\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **Part 4 ‚Äî Advanced Applications**\n",
    "\n",
    "8. **Regularization:**\n",
    "   - Add L2 regularization to your neural network\n",
    "   - Experiment with different regularization strengths\n",
    "   - Observe effects on training vs test performance\n",
    "\n",
    "9. **Weight visualization:**\n",
    "   - Extract and visualize weights from different layers\n",
    "   - Compare weights before and after training\n",
    "   - What patterns do you notice?\n",
    "\n",
    "10. **Custom dataset:**\n",
    "    - Create your own non-linear classification dataset\n",
    "    - Train both linear and neural network models\n",
    "    - Compare performance and decision boundaries\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Goal of These Exercises**\n",
    "\n",
    "By completing these challenges, you will:\n",
    "\n",
    "- ‚úÖ Master the mathematics of neural networks and backpropagation\n",
    "- ‚úÖ Understand how architecture choices affect model performance  \n",
    "- ‚úÖ Build intuition for hyperparameter tuning\n",
    "- ‚úÖ Gain confidence in implementing complex ML algorithms from scratch\n",
    "- ‚úÖ Learn to diagnose and fix common training problems\n",
    "- ‚úÖ **NEW: Appreciate what PyTorch does automatically**\n",
    "- ‚úÖ Prepare for more advanced architectures (CNNs, RNNs, Transformers)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Next Steps\n",
    "\n",
    "In **Lecture 6**, we'll dive into **Convolutional Neural Networks (CNNs)** for image data - using PyTorch!\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Conclusion\n",
    "\n",
    "You've now built **four fundamental ML algorithms from scratch**:\n",
    "1. ‚úÖ Linear Regression (Lecture 1)\n",
    "2. ‚úÖ Binary Logistic Regression (Lecture 4)  \n",
    "3. ‚úÖ Multi-Class Softmax Regression (Lecture 4)\n",
    "4. ‚úÖ Neural Networks (This lecture)\n",
    "\n",
    "You understand:\n",
    "- How gradient descent optimizes complex models\n",
    "- The role of activation functions and non-linearity\n",
    "- How backpropagation efficiently computes gradients\n",
    "- How to implement, visualize, and benchmark neural networks\n",
    "- The mathematical foundations of deep learning\n",
    "- **NEW: How PyTorch automates these processes**\n",
    "\n",
    "**This is the core of modern deep learning!**\n",
    "\n",
    "All advanced architectures (CNNs, RNNs, Transformers) build upon these fundamental concepts.\n",
    "\n",
    "Keep practicing, keep building, and get ready for PyTorch! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final inspirational message\n",
    "print(\"\\n\" + \"‚ú®\" * 70)\n",
    "print(\"üéâ CONGRATULATIONS! You've built neural networks from scratch!\")\n",
    "print(\"üí™ You now understand the fundamentals of deep learning\")\n",
    "print(\"üî¨ You validated your implementation against PyTorch\")\n",
    "print(\"üöÄ Ready to tackle CNNs, RNNs, and beyond WITH PYTORCH!\")\n",
    "print(\"‚ú®\" * 70)\n",
    "\n",
    "print(\"\\nüìö What's Next:\")\n",
    "print(\"‚Ä¢ Lecture 6: Convolutional Neural Networks (PyTorch)\")\n",
    "print(\"‚Ä¢ Lecture 7: Recurrent Neural Networks (PyTorch)\")\n",
    "print(\"‚Ä¢ Lecture 8: Transformers and Attention (PyTorch)\")\n",
    "print(\"‚Ä¢ Lecture 9: Computer Vision Applications\")\n",
    "print(\"‚Ä¢ Lecture 10: Natural Language Processing\")\n",
    "\n",
    "print(\"\\nüåü Remember:\")\n",
    "print(\"Understanding from scratch + PyTorch power = Deep Learning Mastery!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
