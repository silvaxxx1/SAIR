{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Spaceship Titanic: Your First Interstellar ML Mission!\n",
    "\n",
    "## 🌌 Welcome to the Future of Space Travel!\n",
    "\n",
    "Imagine you're a data scientist in the year 2912, where interstellar travel is common. The Spaceship Titanic, an interstellar passenger liner, met a similar fate as its historical counterpart. Your mission: **predict which passengers were transported to an alternate dimension** during the spaceship's collision with a spacetime anomaly!\n",
    "\n",
    "### 🎯 Your Mission Objectives:\n",
    "\n",
    "1. **🕵️ Investigate** the passenger data to find clues about who survived\n",
    "2. **🔧 Build** a machine learning model that can predict survival\n",
    "3. **🚀 Compete** on Kaggle and climb the leaderboard\n",
    "4. **📊 Learn** production-ready ML workflows used by industry professionals\n",
    "\n",
    "> 🎮 **Think of this as a detective game** - you're looking for patterns in the data that reveal who was \"transported\" and who wasn't!\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 What You'll Learn Today\n",
    "\n",
    "| Skill | Why It Matters |\n",
    "|-------|----------------|\n",
    "| **Real-world Data Cleaning** | Real data is messy - learn to handle missing values and outliers |\n",
    "| **Feature Engineering** | Create new features that help your model make better predictions |\n",
    "| **Multiple ML Models** | Compare different algorithms to find the best one |\n",
    "| **Hyperparameter Tuning** | Fine-tune your model like a professional |\n",
    "| **Model Evaluation** | Understand if your model is actually good |\n",
    "| **Kaggle Submission** | Compete with data scientists worldwide |\n",
    "\n",
    "> 💡 **Pro Tip:** Don't just copy the code - try to understand WHY each step matters. This will make you a better data scientist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛠️ Mission Control: Setting Up Your Tools\n",
    "\n",
    "Before we launch into space, let's make sure we have all our tools ready! Think of this as preparing your spaceship for the journey.\n",
    "\n",
    "### Why This Setup Matters:\n",
    "- **Reproducibility**: So you (or others) can get the same results later\n",
    "- **Organization**: Keeping your work clean and professional\n",
    "- **Tracking**: MLflow will be our \"mission log\" tracking everything we do\n",
    "\n",
    "> 🎯 **Learning Goal**: Professional data scientists always set up their environment properly - it saves time and prevents errors later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 LAUNCH SEQUENCE: Importing our space exploration tools!\n",
    "\n",
    "# First, let's silence any annoying warning beeps\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 🌌 Core navigation systems (essential libraries)\n",
    "import numpy as np  # For mathematical calculations\n",
    "import pandas as pd  # For data manipulation - our main tool!\n",
    "import matplotlib.pyplot as plt  # For creating visual charts\n",
    "import seaborn as sns  # For prettier, more informative charts\n",
    "from scipy import stats  # For statistical analysis\n",
    "import joblib  # For saving our trained models\n",
    "import json  # For storing model information\n",
    "from datetime import datetime  # For timestamps\n",
    "import os  # For file operations\n",
    "import time  # For timing our experiments\n",
    "\n",
    "# 🤖 Machine learning crew members\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# 📊 Mission control dashboard (MLflow for tracking)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# 🎨 Enhanced visualization tools\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "print(\"✅ All systems nominal! Mission tools loaded successfully!\")\n",
    "print(\"🚀 Ready to launch our Spaceship Titanic investigation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎛️ Mission Configuration: Setting Our Course\n",
    "\n",
    "Every good space mission needs a flight plan! Let's configure our settings to ensure our journey is smooth and reproducible.\n",
    "\n",
    "### 🔧 Key Configuration Settings:\n",
    "- **RANDOM_STATE = 42**: The \"answer to everything\" - ensures we get the same results every time\n",
    "- **Train/Val/Test Split**: We'll use 60%/20%/20% split to properly evaluate our model\n",
    "- **MLflow Tracking**: Like a mission logbook - tracks every experiment we run\n",
    "\n",
    "> 🎯 **Why This Matters**: Without proper configuration, your results might change every time you run the code - and you won't know why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissionConfig:\n",
    "    \"\"\"Mission Configuration - Our Flight Plan for Success!\"\"\"\n",
    "    \n",
    "    # 🎯 Reproducibility Settings (The \"Secret Sauce\")\n",
    "    RANDOM_STATE = 42  # The answer to life, universe, and reproducible ML!\n",
    "    TEST_SIZE = 0.2    # 20% for final testing - our \"final exam\"\n",
    "    VAL_SIZE = 0.25    # 25% of training data for validation (20% of total)\n",
    "    CV_FOLDS = 5       # 5-fold cross-validation for robust evaluation\n",
    "    N_JOBS = -1        # Use all CPU cores for faster training\n",
    "    \n",
    "    # 📁 Mission Directories (Organizing Our Work)\n",
    "    MODEL_DIR = \"spaceship_models\"\n",
    "    EXPERIMENT_DIR = \"spaceship_experiments\" \n",
    "    SUBMISSION_DIR = \"kaggle_submissions\"\n",
    "    \n",
    "    # 🛠️ Create mission directories\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "    os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "    \n",
    "# Initialize our mission configuration\n",
    "config = MissionConfig()\n",
    "\n",
    "# 📊 Set up MLflow - Our Mission Logbook\n",
    "mlflow.set_tracking_uri(f\"file://{os.path.abspath(config.EXPERIMENT_DIR)}\")\n",
    "experiment_name = \"spaceship_titanic_mission\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(\"🎛️ MISSION CONFIGURATION COMPLETE!\")\n",
    "print(\"📋 Flight Plan Summary:\")\n",
    "print(f\"   • Random Seed: {config.RANDOM_STATE} (for reproducible results)\")\n",
    "print(f\"   • Data Split: 60% Train / 20% Validation / 20% Test\")\n",
    "print(f\"   • Cross-Validation: {config.CV_FOLDS}-fold (robust evaluation)\")\n",
    "print(f\"   • Model Directory: {config.MODEL_DIR}\")\n",
    "print(f\"   • Experiment Tracking: {config.EXPERIMENT_DIR}\")\n",
    "print(\"\\n🚀 All systems go! Ready to load passenger data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📥 Loading Passenger Data: The Investigation Begins!\n",
    "\n",
    "## 🕵️‍♂️ Our Mission: Investigate the Passenger Manifest\n",
    "\n",
    "We have two key files:\n",
    "- `train.csv`: Contains passenger information AND whether they were transported (our training data)\n",
    "- `test.csv`: Contains passenger information ONLY - we need to predict who was transported\n",
    "\n",
    "### 🔍 What We're Looking For:\n",
    "- **Patterns**: What characteristics made someone more likely to be transported?\n",
    "- **Clues**: Missing data, unusual distributions, interesting correlations\n",
    "- **Insights**: Stories hidden in the data that our model can learn from\n",
    "\n",
    "> 🎯 **Detective Mindset**: Approach this like a mystery novel - every column could hold a clue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📂 Loading our passenger manifests\n",
    "print(\"🕵️ Loading Spaceship Titanic Passenger Data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Try to load from local files first\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print(\"✅ Passenger manifests loaded from local files!\")\n",
    "except FileNotFoundError:\n",
    "    # If files aren't found, download from Kaggle\n",
    "    print(\"📥 Downloading passenger data from Kaggle...\")\n",
    "    try:\n",
    "        import kaggle\n",
    "        kaggle.api.competition_download_files('spaceship-titanic', path='.')\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile('spaceship-titanic.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        print(\"✅ Data downloaded and extracted successfully!\")\n",
    "    except:\n",
    "        print(\"❌ Could not download data. Please ensure you have the datasets in your directory.\")\n",
    "        raise\n",
    "\n",
    "# 🎯 Initial Data Reconnaissance\n",
    "print(f\"\\n📊 TRAINING DATA: {train_df.shape[0]} passengers, {train_df.shape[1]} features\")\n",
    "print(f\"📈 TEST DATA: {test_df.shape[0]} passengers, {test_df.shape[1]} features\")\n",
    "\n",
    "print(\"\\n🔍 First Look at Our Passengers:\")\n",
    "print(\"-\" * 50)\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\n📋 Passenger Manifest Overview:\")\n",
    "print(\"-\" * 50)\n",
    "train_df.info()\n",
    "\n",
    "# 🎯 Quick Facts About Our Mission\n",
    "print(f\"\\n🚀 MISSION BRIEFING:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"• Total Passengers to Investigate: {len(train_df) + len(test_df):,}\")\n",
    "print(f\"• Training Samples (Known Outcomes): {len(train_df):,}\")\n",
    "print(f\"• Test Samples (Mystery Cases): {len(test_df):,}\")\n",
    "print(f\"• Features Available: {train_df.shape[1] - 1}\")  # -1 for target\n",
    "print(f\"• Target Variable: 'Transported' (True/False)\")\n",
    "\n",
    "# Store test passenger IDs for our final submission\n",
    "test_passenger_ids = test_df['PassengerId'].copy()\n",
    "print(f\"\\n📝 Saved {len(test_passenger_ids)} passenger IDs for Kaggle submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧹 Data Cleaning: Preparing Our Evidence\n",
    "\n",
    "## 🧼 Why Clean Data Matters:\n",
    "\n",
    "Real-world data is like a crime scene - it's often messy! We need to:\n",
    "- **Handle missing values** (like incomplete witness statements)\n",
    "- **Fix data types** (making sure numbers are numbers, categories are categories)\n",
    "- **Prepare for analysis** (so our models can understand the data)\n",
    "\n",
    "### 🎯 Our Cleaning Strategy:\n",
    "1. **Separate features from target** (what we're trying to predict)\n",
    "2. **Analyze missing data** - where are the gaps in our evidence?\n",
    "3. **Understand our baseline** - what if we just guessed?\n",
    "\n",
    "> 🔍 **Investigator's Note**: Pay attention to missing values - sometimes the pattern of what's missing can be a clue itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧹 STEP 1: Create clean copies of our data\n",
    "print(\"🧹 Beginning Data Cleaning Procedure...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_clean = train_df.copy()\n",
    "test_clean = test_df.copy()\n",
    "\n",
    "# 🎯 Separate our evidence (features) from what we're trying to predict (target)\n",
    "X = train_clean.drop('Transported', axis=1)  # Features - what we know\n",
    "y = train_clean['Transported'].astype(int)   # Target - what we want to predict (convert True/False to 1/0)\n",
    "\n",
    "print(\"✅ Separated features from target variable\")\n",
    "print(f\"   • Features (X): {X.shape[1]} columns\")\n",
    "print(f\"   • Target (y): 1 column ('Transported')\")\n",
    "\n",
    "# 🔍 Investigate Missing Evidence\n",
    "print(\"\\n🔍 ANALYZING MISSING DATA:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "missing_data = X.isnull().sum()\n",
    "missing_percent = (missing_data / len(X)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "# Only show columns with missing data\n",
    "missing_columns = missing_df[missing_df['Missing Count'] > 0]\n",
    "print(f\"📊 Found {len(missing_columns)} features with missing data:\")\n",
    "display(missing_columns)\n",
    "\n",
    "# 🎯 Understanding Our Baseline\n",
    "print(\"\\n🎯 BASELINE ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "transported_stats = y.value_counts()\n",
    "baseline_accuracy = max(transported_stats) / len(y)\n",
    "\n",
    "print(f\"📊 Target Distribution:\")\n",
    "print(f\"   • Transported (1): {transported_stats[1]:,} passengers ({transported_stats[1]/len(y)*100:.1f}%)\")\n",
    "print(f\"   • Not Transported (0): {transported_stats[0]:,} passengers ({transported_stats[0]/len(y)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n💡 BASELINE STRATEGY:\")\n",
    "print(f\"   If we predicted '{'TRANSPORTED' if transported_stats[1] > transported_stats[0] else 'NOT TRANSPORTED'}' for everyone...\")\n",
    "print(f\"   We would be correct {baseline_accuracy:.1%} of the time\")\n",
    "\n",
    "print(f\"\\n🎯 OUR MISSION: Beat the baseline of {baseline_accuracy:.1%} accuracy!\")\n",
    "\n",
    "# 📈 Visualize the target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y, palette=['#ff6b6b', '#51cf66'])\n",
    "plt.title('Passenger Transport Status\\n(0 = Not Transported, 1 = Transported)')\n",
    "plt.xlabel('Transported')\n",
    "plt.ylabel('Number of Passengers')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(transported_stats.values, labels=['Not Transported', 'Transported'], \n",
    "        autopct='%1.1f%%', colors=['#ff6b6b', '#51cf66'], startangle=90)\n",
    "plt.title('Transportation Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Data cleaning phase complete! Ready for deep investigation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Deep Data Investigation: Finding Clues\n",
    "\n",
    "## 🕵️‍♂️ Time to Play Detective!\n",
    "\n",
    "Now we'll explore our data to find patterns and clues. We're looking for:\n",
    "- **Numerical patterns**: Age, spending habits\n",
    "- **Categorical clues**: Home planet, cryosleep status, destination\n",
    "- **Interesting relationships**: How different factors interact\n",
    "\n",
    "### 🎯 Investigation Strategy:\n",
    "1. **Numerical Features**: Distributions and outliers\n",
    "2. **Categorical Features**: Proportions and relationships\n",
    "3. **Spending Patterns**: Who spent money where?\n",
    "\n",
    "> 🔬 **Forensic Mindset**: Look for unusual patterns - they often reveal the most interesting stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎨 Set up our investigation visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"🔍 Beginning Deep Data Investigation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. 🎯 NUMERICAL FEATURES INVESTIGATION\n",
    "print(\"\\n1️⃣ NUMERICAL FEATURES: Age and Spending Habits\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "print(\"📊 Numerical Features Summary:\")\n",
    "numerical_summary = train_df[numerical_features].describe()\n",
    "display(numerical_summary)\n",
    "\n",
    "# Create comprehensive numerical visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    # Create distribution plot\n",
    "    sns.histplot(data=train_df, x=feature, hue='Transported', ax=axes[i], bins=30, alpha=0.7)\n",
    "    axes[i].set_title(f'{feature} Distribution\\n(Colored by Transport Status)')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    \n",
    "    # Add some statistics\n",
    "    transported_mean = train_df[train_df['Transported'] == True][feature].mean()\n",
    "    not_transported_mean = train_df[train_df['Transported'] == False][feature].mean()\n",
    "    \n",
    "    axes[i].axvline(transported_mean, color='blue', linestyle='--', alpha=0.7, label=f'Transported Mean: {transported_mean:.1f}')\n",
    "    axes[i].axvline(not_transported_mean, color='red', linestyle='--', alpha=0.7, label=f'Not Transported Mean: {not_transported_mean:.1f}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. 🎯 CATEGORICAL FEATURES INVESTIGATION  \n",
    "print(\"\\n2️⃣ CATEGORICAL FEATURES: Background and Choices\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    # Calculate transportation rates by category\n",
    "    category_stats = train_df.groupby(feature)['Transported'].agg(['mean', 'count']).reset_index()\n",
    "    category_stats['transport_rate'] = category_stats['mean'] * 100\n",
    "    \n",
    "    # Create bar plot\n",
    "    sns.barplot(data=category_stats, x=feature, y='transport_rate', ax=axes[i], palette='viridis')\n",
    "    axes[i].set_title(f'{feature} vs Transport Rate\\n(Higher = More Likely Transported)')\n",
    "    axes[i].set_ylabel('Transport Rate (%)')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, (_, row) in enumerate(category_stats.iterrows()):\n",
    "        axes[i].text(j, row['transport_rate'] + 1, f\"{row['transport_rate']:.1f}%\", \n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "        axes[i].text(j, -5, f\"n={row['count']}\", ha='center', va='top', fontsize=9, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. 🔍 KEY INSIGHTS AND PATTERNS\n",
    "print(\"\\n3️⃣ KEY INVESTIGATION FINDINGS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate some interesting insights\n",
    "cryosleep_effect = train_df.groupby('CryoSleep')['Transported'].mean()\n",
    "homeplanet_effect = train_df.groupby('HomePlanet')['Transported'].mean()\n",
    "\n",
    "print(\"🔬 INTERESTING PATTERNS FOUND:\")\n",
    "print(f\"   • CryoSleep Effect: {cryosleep_effect[True]:.1%} of cryosleep passengers transported vs {cryosleep_effect[False]:.1%} of awake passengers\")\n",
    "print(f\"   • Home Planet Variations: {homeplanet_effect.idxmax()} has highest transport rate ({homeplanet_effect.max():.1%})\")\n",
    "print(f\"   • Age Pattern: Younger passengers seem slightly more likely to be transported\")\n",
    "print(f\"   • Spending: Passengers with zero spending more likely to be transported\")\n",
    "\n",
    "print(\"\\n🎯 INVESTIGATION COMPLETE! Found several promising clues for our model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎨 Advanced Feature Engineering: Creating Super Clues!\n",
    "\n",
    "## 🧠 Why Feature Engineering Matters:\n",
    "\n",
    "Sometimes the raw data doesn't tell the whole story. We can create **new features** (\"super clues\") that combine information in clever ways!\n",
    "\n",
    "### 🎯 Our Feature Engineering Strategy:\n",
    "1. **Extract group information** from PassengerId\n",
    "2. **Decompose Cabin** into deck, number, and side\n",
    "3. **Create spending features** - total spending and spending indicators\n",
    "4. **Add demographic features** - age groups, family size\n",
    "\n",
    "> 💡 **Creative Insight**: The best features often come from domain knowledge and creative thinking about what might matter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceshipDetective:\n",
    "    \"\"\"\n",
    "    🕵️ Advanced Feature Engineer for Spaceship Titanic\n",
    "    \n",
    "    This class creates powerful new features by combining existing information\n",
    "    in clever ways to help our model find hidden patterns!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting needed for this transformer - it's all based on the data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_eng = X.copy()\n",
    "        \n",
    "        print(\"🛠️ Creating advanced features...\")\n",
    "        \n",
    "        # 🔍 CLUE 1: Extract Group Information from PassengerId\n",
    "        # Format: XXXX_XX where first part is group ID, second is passenger in group\n",
    "        X_eng['GroupId'] = X_eng['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n",
    "        X_eng['GroupSize'] = X_eng.groupby('GroupId')['GroupId'].transform('count')\n",
    "        \n",
    "        # 🔍 CLUE 2: Decompose Cabin into useful parts\n",
    "        # Format: Deck/Number/Side (e.g., B/123/P)\n",
    "        cabin_decomposed = X_eng['Cabin'].str.split('/', expand=True)\n",
    "        X_eng['CabinDeck'] = cabin_decomposed[0]  # Deck level\n",
    "        X_eng['CabinNum'] = pd.to_numeric(cabin_decomposed[1], errors='coerce')  # Cabin number\n",
    "        X_eng['CabinSide'] = cabin_decomposed[2]  # Port (P) or Starboard (S)\n",
    "        \n",
    "        # 🔍 CLUE 3: Create Spending Features\n",
    "        spending_features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "        X_eng['TotalSpending'] = X_eng[spending_features].sum(axis=1)\n",
    "        X_eng['HasSpending'] = (X_eng['TotalSpending'] > 0).astype(int)\n",
    "        \n",
    "        # 🔍 CLUE 4: Create Demographic Features\n",
    "        # Age groups that might behave differently\n",
    "        X_eng['AgeGroup'] = pd.cut(X_eng['Age'], \n",
    "                                  bins=[0, 12, 18, 30, 50, 100], \n",
    "                                  labels=['Child', 'Teen', 'Young Adult', 'Adult', 'Senior'])\n",
    "        \n",
    "        # Family/Social features\n",
    "        X_eng['IsAlone'] = (X_eng['GroupSize'] == 1).astype(int)\n",
    "        X_eng['IsChild'] = (X_eng['Age'] < 13).astype(int)\n",
    "        X_eng['IsElderly'] = (X_eng['Age'] > 60).astype(int)\n",
    "        \n",
    "        # 🔍 CLUE 5: Create Interaction Features\n",
    "        # Wealth indicator: VIP status combined with spending\n",
    "        X_eng['WealthIndicator'] = X_eng['VIP'].astype(int) * X_eng['TotalSpending']\n",
    "        \n",
    "        # 🔍 CLUE 6: Spending Ratios\n",
    "        # What percentage of total spending goes to luxury vs necessities?\n",
    "        X_eng['LuxuryRatio'] = (X_eng['RoomService'] + X_eng['Spa'] + X_eng['VRDeck']) / (X_eng['TotalSpending'] + 1)\n",
    "        X_eng['FoodRatio'] = X_eng['FoodCourt'] / (X_eng['TotalSpending'] + 1)\n",
    "        \n",
    "        # 🧹 Clean up: Remove original columns we've decomposed\n",
    "        columns_to_drop = ['PassengerId', 'Cabin', 'Name']\n",
    "        X_eng = X_eng.drop([col for col in columns_to_drop if col in X_eng.columns], axis=1)\n",
    "        \n",
    "        # 📝 Track our new feature names\n",
    "        self.feature_names = list(X_eng.columns)\n",
    "        \n",
    "        return X_eng\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "# 🚀 Let's test our feature engineering!\n",
    "print(\"🎨 Starting Advanced Feature Engineering...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "detective = SpaceshipDetective()\n",
    "X_engineered = detective.fit_transform(X)\n",
    "\n",
    "print(f\"\\n✅ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(f\"📊 Original features: {X.shape[1]}\")\n",
    "print(f\"🚀 Engineered features: {X_engineered.shape[1]}\")\n",
    "print(f\"🎯 New features created: {X_engineered.shape[1] - X.shape[1]}\")\n",
    "\n",
    "# Show our new features\n",
    "new_features = [f for f in X_engineered.columns if f not in X.columns]\n",
    "print(f\"\\n🔍 NEW FEATURES CREATED:\")\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "print(\"\\n📋 Preview of Engineered Data:\")\n",
    "print(\"-\" * 50)\n",
    "display(X_engineered.head())\n",
    "\n",
    "print(\"\\n🎉 Feature engineering successful! Our model now has powerful new clues to work with!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏗️ Building Our Preprocessing Pipeline\n",
    "\n",
    "## 🛠️ Why Pipelines Matter:\n",
    "\n",
    "A pipeline is like an assembly line for your data - it automatically handles:\n",
    "- **Missing values** (filling in gaps)\n",
    "- **Scaling** (making sure all features play fair)\n",
    "- **Encoding** (converting categories to numbers)\n",
    "\n",
    "### 🎯 Pipeline Benefits:\n",
    "- **Reproducible**: Same steps every time\n",
    "- **Prevents Data Leakage**: No information from test set leaks into training\n",
    "- **Easy Deployment**: One object handles all preprocessing\n",
    "\n",
    "> 💡 **Pro Tip**: Pipelines are essential for professional ML workflows - they prevent common mistakes and save time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏗️ Building Advanced Preprocessing Pipeline...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 🔍 Identify different types of features\n",
    "numerical_features = X_engineered.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_engineered.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"📊 FEATURE ANALYSIS:\")\n",
    "print(f\"   • Numerical Features: {len(numerical_features)}\")\n",
    "print(f\"   • Categorical Features: {len(categorical_features)}\")\n",
    "print(f\"   • Total Features: {len(numerical_features) + len(categorical_features)}\")\n",
    "\n",
    "# 🛠️ Build Numerical Pipeline\n",
    "print(\"\\n🛠️ Building Numerical Pipeline...\")\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Fill missing values with median\n",
    "    ('scaler', RobustScaler())  # Scale features, robust to outliers\n",
    "])\n",
    "\n",
    "# 🛠️ Build Categorical Pipeline  \n",
    "print(\"🛠️ Building Categorical Pipeline...\")\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing with most common category\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Convert categories to numbers\n",
    "])\n",
    "\n",
    "# 🎯 Create Column Transformer (applies different pipelines to different columns)\n",
    "print(\"🎯 Creating Column Transformer...\")\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# 🚀 Create Full Pipeline (Feature Engineering + Preprocessing)\n",
    "print(\"🚀 Creating Full Pipeline...\")\n",
    "full_pipeline = Pipeline([\n",
    "    ('feature_engineer', SpaceshipDetective()),\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# 🔧 Apply our pipeline to the data\n",
    "print(\"\\n🔧 Applying pipeline to training data...\")\n",
    "X_processed = full_pipeline.fit_transform(X, y)\n",
    "\n",
    "print(f\"\\n✅ PIPELINE CONSTRUCTION COMPLETE!\")\n",
    "print(f\"📊 Final processed data shape: {X_processed.shape}\")\n",
    "print(f\"🎯 Pipeline steps:\")\n",
    "for i, (name, step) in enumerate(full_pipeline.steps):\n",
    "    print(f\"   {i+1}. {name}: {step.__class__.__name__}\")\n",
    "\n",
    "print(\"\\n💡 Pipeline is ready! It will automatically handle all preprocessing for new data too!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧪 Train-Validation-Test Split: Setting Up Our Experiments\n",
    "\n",
    "## 🎯 Why We Split Data:\n",
    "\n",
    "We need to test our model on data it hasn't seen before to make sure it actually works! Think of it like:\n",
    "- **Training Set**: Textbook (what the model studies)\n",
    "- **Validation Set**: Practice exams (how we tune the model)\n",
    "- **Test Set**: Final exam (true measure of performance)\n",
    "\n",
    "### 📊 Our Split Strategy:\n",
    "- **60% Training**: For the model to learn patterns\n",
    "- **20% Validation**: For tuning hyperparameters\n",
    "- **20% Test**: For final evaluation (never touch until the end!)\n",
    "\n",
    "> 🚫 **Critical Rule**: Never let the model see the test set during training - that's cheating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Setting Up Data Splits for Robust Evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First split: Separate test set (our \"final exam\")\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_processed, y, \n",
    "    test_size=config.TEST_SIZE, \n",
    "    random_state=config.RANDOM_STATE, \n",
    "    stratify=y  # Keep same proportion of transported/not transported in each split\n",
    ")\n",
    "\n",
    "# Second split: Separate validation set (our \"practice exams\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=config.VAL_SIZE, \n",
    "    random_state=config.RANDOM_STATE, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"✅ DATA SPLITS CREATED SUCCESSFULLY!\")\n",
    "print(\"\\n📊 SPLIT SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"🏫 TRAINING SET: {X_train.shape[0]:,} passengers\")\n",
    "print(f\"   • Used for: Teaching the model patterns\")\n",
    "print(f\"   • Transported: {y_train.sum():,} ({y_train.mean():.1%})\")\n",
    "\n",
    "print(f\"\\n📝 VALIDATION SET: {X_val.shape[0]:,} passengers\")  \n",
    "print(f\"   • Used for: Tuning model parameters\")\n",
    "print(f\"   • Transported: {y_val.sum():,} ({y_val.mean():.1%})\")\n",
    "\n",
    "print(f\"\\n🎯 TEST SET: {X_test.shape[0]:,} passengers\")\n",
    "print(f\"   • Used for: Final evaluation (KEPT SECRET!)\")\n",
    "print(f\"   • Transported: {y_test.sum():,} ({y_test.mean():.1%})\")\n",
    "\n",
    "print(f\"\\n📈 TOTAL DATA: {X_processed.shape[0]:,} passengers\")\n",
    "print(f\"   • Features: {X_processed.shape[1]}\")\n",
    "print(f\"   • Transported Overall: {y.sum():,} ({y.mean():.1%})\")\n",
    "\n",
    "# Visualize our splits\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "split_data = [\n",
    "    ('Training', len(X_train), '#51cf66'),\n",
    "    ('Validation', len(X_val), '#ffd43b'), \n",
    "    ('Test', len(X_test), '#ff6b6b')\n",
    "]\n",
    "\n",
    "names, sizes, colors = zip(*split_data)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(sizes, labels=names, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Data Split Proportions')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "bars = plt.bar(names, sizes, color=colors)\n",
    "plt.title('Data Split Sizes')\n",
    "plt.ylabel('Number of Passengers')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "             f'{size:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🚀 Data splits ready! Time to train some models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 Assembling Our Model Team\n",
    "\n",
    "## 🎯 Why Multiple Models Matter:\n",
    "\n",
    "Different models have different strengths - like having a team of specialists!\n",
    "\n",
    "| Model | Speciality | Best For |\n",
    "|-------|------------|----------|\n",
    "| **Logistic Regression** | Simple, interpretable | Linear relationships, baseline |\n",
    "| **Random Forest** | Robust, handles mixed data | Complex patterns, feature importance |\n",
    "| **Gradient Boosting** | High accuracy, sequential learning | Winning competitions! |\n",
    "| **SVM** | Complex boundaries | High-dimensional spaces |\n",
    "| **K-Neighbors** | Instance-based | Local patterns, similar passengers |\n",
    "| **Voting Ensemble** | Wisdom of crowds | Combining strengths of multiple models |\n",
    "\n",
    "> 🎯 **Strategy**: We'll try them all and see which one performs best on our validation set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🤖 Assembling Our Machine Learning Team...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define our model team\n",
    "model_team = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=config.RANDOM_STATE, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=config.RANDOM_STATE, n_jobs=config.N_JOBS),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=config.RANDOM_STATE),\n",
    "    'Support Vector Machine': SVC(random_state=config.RANDOM_STATE, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_jobs=config.N_JOBS),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=config.RANDOM_STATE),\n",
    "}\n",
    "\n",
    "# Create a super-team: Voting Ensemble\n",
    "print(\"🌟 Creating Super Team: Voting Ensemble...\")\n",
    "voting_ensemble = VotingClassifier([\n",
    "    ('rf', RandomForestClassifier(random_state=config.RANDOM_STATE, n_jobs=config.N_JOBS)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=config.RANDOM_STATE)),\n",
    "    ('lr', LogisticRegression(random_state=config.RANDOM_STATE, max_iter=1000))\n",
    "], voting='soft')  # 'soft' = use probabilities instead of hard votes\n",
    "\n",
    "model_team['Voting Ensemble'] = voting_ensemble\n",
    "\n",
    "print(\"✅ MODEL TEAM ASSEMBLED!\")\n",
    "print(f\"\\n👥 TEAM MEMBERS: {len(model_team)} models\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, (name, model) in enumerate(model_team.items(), 1):\n",
    "    print(f\"{i:2d}. {name:25} - {model.__class__.__name__}\")\n",
    "\n",
    "print(f\"\\n🎯 STRATEGY: Train all models, then select the best performer!\")\n",
    "print(\"💡 We'll use MLflow to track every experiment automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Model Training & Evaluation: The Grand Tournament!\n",
    "\n",
    "## 🏆 How We'll Judge Our Models:\n",
    "\n",
    "We'll use multiple metrics to get a complete picture:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: When we say \"transported\", how often are we right?\n",
    "- **Recall**: How many actual transported passengers do we catch?\n",
    "- **F1-Score**: Balance between precision and recall\n",
    "- **ROC-AUC**: Overall model performance across all thresholds\n",
    "\n",
    "> 📊 **MLflow Magic**: Every model, every parameter, every result will be automatically tracked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_tournament(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"\n",
    "    🏆 Run a complete model evaluation with comprehensive tracking\n",
    "    \n",
   "    This function trains a model, evaluates it on multiple metrics,\n",
    "    and logs everything to MLflow for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # ⏱️ Track training time\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # 🔮 Make predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_val_proba = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # 📊 Calculate comprehensive metrics\n",
    "        metrics = {\n",
    "            'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'val_accuracy': accuracy_score(y_val, y_val_pred),\n",
    "            'train_precision': precision_score(y_train, y_train_pred),\n",
    "            'val_precision': precision_score(y_val, y_val_pred),\n",
    "            'train_recall': recall_score(y_train, y_train_pred),\n",
    "            'val_recall': recall_score(y_val, y_val_pred),\n",
    "            'train_f1': f1_score(y_train, y_train_pred),\n",
    "            'val_f1': f1_score(y_val, y_val_pred),\n",
    "            'training_time': training_time,\n",
    "            'overfitting_gap': accuracy_score(y_train, y_train_pred) - accuracy_score(y_val, y_val_pred)\n",
    "        }\n",
    "        \n",
    "        # Add ROC-AUC if we have probabilities\n",
    "        if y_val_proba is not None:\n",
    "            metrics['val_roc_auc'] = roc_auc_score(y_val, y_val_proba)\n",
    "        \n",
    "        # 📝 Log to MLflow (our experiment tracker)\n",
    "        mlflow.log_params(model.get_params())\n",
    "        mlflow.log_metrics({k: v for k, v in metrics.items() if k != 'training_time'})\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # 🔄 Cross-validation for robust performance estimate\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, \n",
    "                                  cv=config.CV_FOLDS, scoring='accuracy', \n",
    "                                  n_jobs=config.N_JOBS)\n",
    "        metrics['cv_accuracy_mean'] = cv_scores.mean()\n",
    "        metrics['cv_accuracy_std'] = cv_scores.std()\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            'cv_accuracy_mean': metrics['cv_accuracy_mean'],\n",
    "            'cv_accuracy_std': metrics['cv_accuracy_std']\n",
    "        })\n",
    "        \n",
    "        return metrics, model\n",
    "\n",
    "# 🚀 Start the model tournament!\n",
    "print(\"🚀 STARTING MODEL TOURNAMENT!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"🏆 Each model will be trained and evaluated...\")\n",
    "print(\"📊 MLflow is tracking all experiments automatically\\n\")\n",
    "\n",
    "tournament_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in model_team.items():\n",
    "    print(f\"🔧 Training {name}...\")\n",
    "    metrics, trained_model = run_model_tournament(\n",
    "        model, X_train, X_val, y_train, y_val, name\n",
    "    )\n",
    "    tournament_results[name] = metrics\n",
    "    trained_models[name] = trained_model\n",
    "    \n",
    "    # Show quick results\n",
    "    overfitting_indicator = \"⚠️\" if metrics['overfitting_gap'] > 0.1 else \"✅\"\n",
    "    print(f\"   ✅ {name:25} | Val Acc: {metrics['val_accuracy']:.4f} | \"\n",
    "          f\"CV Acc: {metrics['cv_accuracy_mean']:.4f} ± {metrics['cv_accuracy_std']:.4f} \"\n",
    "          f\"{overfitting_indicator}\")\n",
    "\n",
    "print(f\"\\n🎉 TOURNAMENT COMPLETE!\")\n",
    "print(f\"💡 View detailed results: mlflow ui --backend-store-uri {config.EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Tournament Results: And the Winner Is...\n",
    "\n",
    "## 🏆 Time to Crown Our Champion!\n",
    "\n",
    "Let's analyze all the models and select the best one based on:\n",
    "- **Validation Accuracy**: Performance on unseen data\n",
    "- **Cross-Validation**: Robustness across different data splits\n",
    "- **Overfitting**: Gap between training and validation performance\n",
    "- **Training Time**: Computational efficiency\n",
    "\n",
    "> 🎯 **Goal**: Find the model that generalizes best to new, unseen data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Analyze tournament results\n",
    "print(\"📊 ANALYZING TOURNAMENT RESULTS...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame(tournament_results).T\n",
    "results_df = results_df.sort_values('val_accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n🏆 FINAL TOURNAMENT STANDINGS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Rank':<4} {'Model':<22} {'Val Acc':<8} {'CV Acc':<12} {'Overfit':<10} {'Time (s)':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for rank, (model_name, row) in enumerate(results_df.iterrows(), 1):\n",
    "    overfitting_indicator = \"⚠️ HIGH\" if row['overfitting_gap'] > 0.1 else \"✅ LOW\"\n",
    "    medal = \"🥇\" if rank == 1 else \"🥈\" if rank == 2 else \"🥉\" if rank == 3 else \"  \"\n",
    "    \n",
    "    print(f\"{medal} {rank:<2} {model_name:<22} {row['val_accuracy']:>7.4f} \"\n",
    "          f\"{row['cv_accuracy_mean']:>7.4f} ± {row['cv_accuracy_std']:>5.4f} \"\n",
    "          f\"{overfitting_indicator:<10} {row['training_time']:>9.2f}\")\n",
    "\n",
    "# 🎯 Select the champion\n",
    "champion_name = results_df.index[0]\n",
    "champion_model = trained_models[champion_name]\n",
    "\n",
    "print(f\"\\n🎉 TOURNAMENT CHAMPION: {champion_name}!\")\n",
    "print(f\"📊 Validation Accuracy: {results_df.iloc[0]['val_accuracy']:.4f}\")\n",
    "print(f\"🔍 CV Accuracy: {results_df.iloc[0]['cv_accuracy_mean']:.4f} ± {results_df.iloc[0]['cv_accuracy_std']:.4f}\")\n",
    "\n",
    "# 📊 Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "models_ordered = results_df.index\n",
    "val_acc = [tournament_results[model]['val_accuracy'] for model in models_ordered]\n",
    "train_acc = [tournament_results[model]['train_accuracy'] for model in models_ordered]\n",
    "\n",
    "x = np.arange(len(models_ordered))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0, 0].bar(x - width/2, train_acc, width, label='Train Accuracy', alpha=0.7, color='skyblue')\n",
    "bars2 = axes[0, 0].bar(x + width/2, val_acc, width, label='Val Accuracy', alpha=0.7, color='lightcoral')\n",
    "axes[0, 0].set_xlabel('Models')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Training vs Validation Accuracy\\n(Small gap = good generalization)')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(models_ordered, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. F1-Score Comparison\n",
    "val_f1 = [tournament_results[model]['val_f1'] for model in models_ordered]\n",
    "bars = axes[0, 1].bar(models_ordered, val_f1, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_xlabel('Models')\n",
    "axes[0, 1].set_ylabel('F1-Score')\n",
    "axes[0, 1].set_title('Validation F1-Score\\n(Balances precision and recall)')\n",
    "axes[0, 1].set_xticklabels(models_ordered, rotation=45, ha='right')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, val_f1):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Training Time Comparison\n",
    "training_times = [tournament_results[model]['training_time'] for model in models_ordered]\n",
    "bars = axes[1, 0].bar(models_ordered, training_times, alpha=0.7, color='orange')\n",
    "axes[1, 0].set_xlabel('Models')\n",
    "axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "axes[1, 0].set_title('Computational Efficiency\\n(Faster training = more experimentation)')\n",
    "axes[1, 0].set_xticklabels(models_ordered, rotation=45, ha='right')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ROC-AUC Comparison\n",
    "roc_auc_scores = []\n",
    "model_names_roc = []\n",
    "for model_name in models_ordered:\n",
    "    if 'val_roc_auc' in tournament_results[model_name]:\n",
    "        roc_auc_scores.append(tournament_results[model_name]['val_roc_auc'])\n",
    "        model_names_roc.append(model_name)\n",
    "\n",
    "if roc_auc_scores:\n",
    "    bars = axes[1, 1].bar(model_names_roc, roc_auc_scores, alpha=0.7, color='purple')\n",
    "    axes[1, 1].set_xlabel('Models')\n",
    "    axes[1, 1].set_ylabel('ROC-AUC Score')\n",
    "    axes[1, 1].set_title('ROC-AUC Comparison\\n(Higher = better overall performance)')\n",
    "    axes[1, 1].set_xticklabels(model_names_roc, rotation=45, ha='right')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, roc_auc_scores):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 ANALYSIS COMPLETE! We have our champion model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚙️ Hyperparameter Tuning: Making Our Champion Even Stronger!\n",
    "\n",
    "## 🎯 What is Hyperparameter Tuning?\n",
    "\n",
    "Think of hyperparameters as the \"knobs and dials\" on our model. Tuning them is like:\n",
    "- **Fine-tuning a radio** to get the clearest signal\n",
    "- **Adjusting a recipe** to make it taste perfect\n",
    "- **Optimizing a car** for peak performance\n",
    "\n",
    "### 🚀 Our Tuning Strategy:\n",
    "We'll use **RandomizedSearchCV** which tries random combinations of hyperparameters - it's faster than trying every single combination!\n",
    "\n",
    "> 💡 **Pro Tip**: Tuning can often improve model performance by 2-5% - that's huge in competitions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Define hyperparameter search spaces for our top models\n",
    "hyperparameter_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300, 400],  # Number of trees\n",
    "        'max_depth': [None, 10, 20, 30],       # How deep each tree can grow\n",
    "        'min_samples_split': [2, 5, 10],       # Minimum samples to split a node\n",
    "        'min_samples_leaf': [1, 2, 4],         # Minimum samples at a leaf node\n",
    "        'max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider for splits\n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],        # Number of boosting stages\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],  # How much each tree contributes\n",
    "        'max_depth': [3, 4, 5, 6],             # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5, 10],       # Minimum samples to split\n",
    "        'subsample': [0.8, 0.9, 1.0]           # Fraction of samples used for fitting\n",
    "    },\n",
    "    \n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],       # Type of regularization\n",
    "        'solver': ['liblinear', 'saga']              # Optimization algorithm\n",
    "    }\n",
    "}\n",
    "\n",
    "# 🚀 Start hyperparameter tuning for top models\n",
    "print(\"⚙️ STARTING HYPERPARAMETER TUNING...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"🎯 Tuning top 3 models from our tournament...\\n\")\n",
    "\n",
    "tuned_champions = {}\n",
    "tuning_results = {}\n",
    "\n",
    "# Tune the top 3 models\n",
    "top_models_to_tune = results_df.index[:3]\n",
    "\n",
    "for model_name in top_models_to_tune:\n",
    "    if model_name in hyperparameter_grids:\n",
    "        print(f\"🔧 Tuning {model_name}...\")\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"{model_name}_TUNED\"):\n",
    "            # Use RandomizedSearchCV for efficient hyperparameter search\n",
    "            search = RandomizedSearchCV(\n",
    "                model_team[model_name],           # The model to tune\n",
    "                hyperparameter_grids[model_name], # Parameter grid to search\n",
    "                n_iter=20,                       # Try 20 random combinations\n",
    "                cv=config.CV_FOLDS,              # 5-fold cross-validation\n",
    "                scoring='accuracy',              # Optimize for accuracy\n",
    "                n_jobs=config.N_JOBS,            # Use all CPU cores\n",
    "                random_state=config.RANDOM_STATE,# For reproducibility\n",
    "                verbose=1                        # Show progress\n",
    "            )\n",
    "            \n",
    "            # 🔍 Perform the search!\n",
    "            search.fit(X_train, y_train)\n",
    "            \n",
    "            # 💾 Store the tuned champion\n",
    "            tuned_champions[model_name] = search.best_estimator_\n",
    "            tuning_results[model_name] = {\n",
    "                'best_score': search.best_score_,\n",
    "                'best_params': search.best_params_,\n",
    "                'best_estimator': search.best_estimator_\n",
    "            }\n",
    "            \n",
    "            # 📝 Log everything to MLflow\n",
    "            mlflow.log_params(search.best_params_)\n",
    "            mlflow.log_metric('best_cv_score', search.best_score_)\n",
    "            mlflow.sklearn.log_model(search.best_estimator_, \"tuned_model\")\n",
    "            \n",
    "            print(f\"   ✅ {model_name:20} | Best CV Accuracy: {search.best_score_:.4f}\")\n",
    "            print(f\"      Improvement: +{search.best_score_ - tournament_results[model_name]['cv_accuracy_mean']:.4f}\")\n",
    "            print(f\"      Best parameters: {search.best_params_}\")\n",
    "\n",
    "print(f\"\\n🎉 HYPERPARAMETER TUNING COMPLETE!\")\n",
    "print(\"💡 Our models are now optimized for peak performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏆 Final Model Selection: Choosing Our Ultimate Champion\n",
    "\n",
    "## 🎯 Time for the Final Decision!\n",
    "\n",
    "We'll evaluate our tuned models on the validation set and select the absolute best performer. Then we'll do a final evaluation on the **test set** (which we've kept completely separate until now!).\n",
    "\n",
    "### 🔬 Our Evaluation Criteria:\n",
    "- **Validation Accuracy**: Primary metric\n",
    "- **F1-Score**: Balance of precision and recall  \n",
    "- **ROC-AUC**: Overall model performance\n",
    "- **Stability**: Consistent performance across metrics\n",
    "\n",
    "> 🚨 **Important**: This is our last chance to choose before the final test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏆 Evaluate tuned champions on validation set\n",
    "print(\"🏆 FINAL MODEL SELECTION ROUND...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_results = {}\n",
    "for model_name, tuned_model in tuned_champions.items():\n",
    "    # Make predictions on validation set\n",
    "    y_val_pred = tuned_model.predict(X_val)\n",
    "    y_val_proba = tuned_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "    \n",
    "    # Compare with untuned version\n",
    "    improvement = val_accuracy - tournament_results[model_name]['val_accuracy']\n",
    "    \n",
    "    final_results[model_name] = {\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_f1': val_f1,\n",
    "        'val_roc_auc': val_roc_auc,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "\n",
    "# 🎯 Print comparison between tuned and untuned\n",
    "print(\"\\n📊 TUNED vs UNTUNED PERFORMANCE:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Model':<20} {'Untuned Acc':<12} {'Tuned Acc':<12} {'Improvement':<12} {'F1-Score':<10} {'ROC-AUC':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_name, res in final_results.items():\n",
    "    untuned_acc = tournament_results[model_name]['val_accuracy']\n",
    "    tuned_acc = res['val_accuracy']\n",
    "    improvement = res['improvement']\n",
    "    \n",
    "    icon = \"📈\" if improvement > 0 else \"📉\" if improvement < 0 else \"➡️\"\n",
    "    \n",
    "    print(f\"{model_name:<20} {untuned_acc:>10.4f} {tuned_acc:>10.4f} {icon} {improvement:>8.4f} \"\n",
    "          f\"{res['val_f1']:>10.4f} {res['val_roc_auc']:>10.4f}\")\n",
    "\n",
    "# 🏅 Select the ultimate champion\n",
    "ultimate_champion_name = max(final_results, key=lambda m: final_results[m]['val_accuracy'])\n",
    "ultimate_champion = tuned_champions[ultimate_champion_name]\n",
    "\n",
    "print(f\"\\n🎉 ULTIMATE CHAMPION SELECTED: {ultimate_champion_name}!\")\n",
    "print(f\"📊 Validation Accuracy: {final_results[ultimate_champion_name]['val_accuracy']:.4f}\")\n",
    "print(f\"📈 Improvement from tuning: +{final_results[ultimate_champion_name]['improvement']:.4f}\")\n",
    "print(f\"🎯 F1-Score: {final_results[ultimate_champion_name]['val_f1']:.4f}\")\n",
    "print(f\"📊 ROC-AUC: {final_results[ultimate_champion_name]['val_roc_auc']:.4f}\")\n",
    "\n",
    "# 🧪 FINAL TEST SET EVALUATION (The moment of truth!)\n",
    "print(f\"\\n🧪 FINAL TEST SET EVALUATION:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "y_test_pred = ultimate_champion.predict(X_test)\n",
    "y_test_proba = ultimate_champion.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(f\"📊 Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"🎯 Test Precision: {test_precision:.4f}\")\n",
    "print(f\"🔍 Test Recall: {test_recall:.4f}\")\n",
    "print(f\"⚖️ Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"📈 Test ROC-AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "baseline_accuracy = max(y_test.value_counts()) / len(y_test)\n",
    "improvement_over_baseline = test_accuracy - baseline_accuracy\n",
    "\n",
    "print(f\"\\n💡 IMPROVEMENT OVER BASELINE: +{improvement_over_baseline:.4f} ({improvement_over_baseline/.01:.1f}% relative improvement!)\")\n",
    "\n",
    "# 📊 Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Transported', 'Transported'],\n",
    "            yticklabels=['Not Transported', 'Transported'])\n",
    "plt.title('Confusion Matrix - Test Set\\n(How our model performed)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "metrics_values = [test_accuracy, test_precision, test_recall, test_f1, test_roc_auc]\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "colors = ['#51cf66', '#ffd43b', '#ffa94d', '#ff6b6b', '#748ffc']\n",
    "\n",
    "bars = plt.bar(metrics_names, metrics_values, color=colors, alpha=0.7)\n",
    "plt.title('Test Set Performance Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 📋 Detailed Classification Report\n",
    "print(\"\\n📋 DETAILED CLASSIFICATION REPORT:\")\n",
    "print(\"-\" * 50)\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                          target_names=['Not Transported', 'Transported']))\n",
    "\n",
    "print(\"\\n🎉 FINAL EVALUATION COMPLETE! Our model is ready for Kaggle!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Preparing for Kaggle Submission\n",
    "\n",
    "## 📤 Getting Ready for the Competition!\n",
    "\n",
    "Now we'll:\n",
    "1. **Retrain our champion** on all available training data\n",
    "2. **Generate predictions** for the test set\n",
    "3. **Create the submission file** in the proper format\n",
    "4. **Save all our work** for future use\n",
    "\n",
    "### 🎯 Submission Format:\n",
    "We need a CSV with two columns:\n",
    "- `PassengerId`: The passenger identifier\n",
    "- `Transported`: `True` or `False` predictions\n",
    "\n",
    "> 💡 **Pro Tip**: Always retrain your final model on all available data before submission - more data usually means better performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Retrain champion on full training data\n",
    "print(\"🚀 PREPARING KAGGLE SUBMISSION...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"🔄 Retraining ultimate champion on full training data...\")\n",
    "X_full_train = np.vstack([X_train, X_val])\n",
    "y_full_train = np.concatenate([y_train, y_val])\n",
    "\n",
    "final_champion = ultimate_champion\n",
    "final_champion.fit(X_full_train, y_full_train)\n",
    "\n",
    "print(\"✅ Model retrained on full dataset\")\n",
    "print(f\"   • Training samples: {X_full_train.shape[0]:,}\")\n",
    "print(f\"   • Features: {X_full_train.shape[1]}\")\n",
    "\n",
    "# 🔮 Generate predictions for test set\n",
    "print(\"\\n🔮 Generating predictions for competition test set...\")\n",
    "test_processed = full_pipeline.transform(test_df)\n",
    "test_predictions = final_champion.predict(test_processed)\n",
    "test_probabilities = final_champion.predict_proba(test_processed)[:, 1]\n",
    "\n",
    "print(\"✅ Predictions generated\")\n",
    "print(f\"   • Test passengers: {len(test_predictions):,}\")\n",
    "print(f\"   • Predicted transported: {test_predictions.sum():,} ({test_predictions.mean():.1%})\")\n",
    "\n",
    "# 📝 Create submission DataFrame\n",
    "print(\"\\n📝 Creating submission file...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Transported': test_predictions.astype(bool)  # Convert back to True/False\n",
    "})\n",
    "\n",
    "# 💾 Save submission file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_filename = f\"submission_{timestamp}.csv\"\n",
    "submission_path = os.path.join(config.SUBMISSION_DIR, submission_filename)\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission file saved: {submission_path}\")\n",
    "\n",
    "# 📊 Analyze our submission\n",
    "print(\"\\n📊 SUBMISSION ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"📁 File: {submission_filename}\")\n",
    "print(f\"📏 Shape: {submission_df.shape}\")\n",
    "print(f\"🎯 Prediction Distribution:\")\n",
    "prediction_counts = submission_df['Transported'].value_counts()\n",
    "print(f\"   • Transported (True): {prediction_counts[True]:,} ({prediction_counts[True]/len(submission_df)*100:.1f}%)\")\n",
    "print(f\"   • Not Transported (False): {prediction_counts[False]:,} ({prediction_counts[False]/len(submission_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n📋 Submission Preview:\")\n",
    "print(\"-\" * 40)\n",
    "display(submission_df.head(10))\n",
    "\n",
    "print(\"\\n🎉 SUBMISSION READY! Upload to Kaggle and see your score!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💾 Saving Our Work: Model Versioning & Artifacts\n",
    "\n",
    "## 🏗️ Professional ML Workflow: Saving Everything\n",
    "\n",
    "A professional data scientist always saves:\n",
    "- **The trained model** for future predictions\n",
    "- **The preprocessing pipeline** to handle new data the same way\n",
    "- **Model metadata** (performance, parameters, etc.)\n",
    "- **Experiment tracking** with MLflow\n",
    "\n",
    "### 🎯 Why This Matters:\n",
    "- **Reproducibility**: You or others can reproduce your work\n",
    "- **Deployment**: Ready to use in applications\n",
    "- **Iteration**: Build upon your work later\n",
    "\n",
    "> 📚 **Best Practice**: Always version your models and keep detailed documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Save everything for future use\n",
    "print(\"💾 SAVING MODEL ARTIFACTS...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create versioned model directory\n",
    "model_version = f\"champion_v1_{timestamp}\"\n",
    "model_save_dir = os.path.join(config.MODEL_DIR, model_version)\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"📁 Creating model directory: {model_save_dir}\")\n",
    "\n",
    "# 1. Save the trained model\n",
    "model_path = os.path.join(model_save_dir, 'spaceship_model.pkl')\n",
    "joblib.dump(final_champion, model_path)\n",
    "print(f\"✅ Model saved: {model_path}\")\n",
    "\n",
    "# 2. Save the preprocessing pipeline\n",
    "pipeline_path = os.path.join(model_save_dir, 'preprocessing_pipeline.pkl')\n",
    "joblib.dump(full_pipeline, pipeline_path)\n",
    "print(f\"✅ Preprocessing pipeline saved: {pipeline_path}\")\n",
    "\n",
    "# 3. Create and save model card (metadata)\n",
    "model_card = {\n",
    "    'model_name': ultimate_champion_name,\n",
    "    'model_version': model_version,\n",
    "    'timestamp': timestamp,\n",
    "    'dataset': 'Spaceship Titanic',\n",
    "    'task': 'Binary Classification',\n",
    "    'target': 'Transported',\n",
    "    'author': 'Data Science Student',\n",
    "    \n",
    'performance': {\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_precision': float(test_precision),\n",
    "        'test_recall': float(test_recall),\n",
    "        'test_f1': float(test_f1),\n",
    "        'test_roc_auc': float(test_roc_auc),\n",
    "        'val_accuracy': float(final_results[ultimate_champion_name]['val_accuracy']),\n",
    "        'baseline_accuracy': float(baseline_accuracy),\n",
    "        'improvement_over_baseline': float(improvement_over_baseline)\n",
    "    },\n",
    "    \n",
    "    'data_info': {\n",
    "        'train_samples': int(len(X_full_train)),\n",
    "        'test_samples': int(len(test_processed)),\n",
    "        'n_features': X_full_train.shape[1],\n",
    "        'feature_engineer': 'SpaceshipDetective',\n",
    "        'new_features_created': len([f for f in detective.get_feature_names() if f not in X.columns])\n",
    "    },\n",
    "    \n",
    "    'model_config': {\n",
    "        'model_class': ultimate_champion_name,\n",
    "        'hyperparameters': dict(final_champion.get_params()),\n",
    "    },\n",
    "    \n",
    "    'preprocessing': {\n",
    "        'steps': [\n",
    "            'SpaceshipDetective (advanced feature engineering)',\n",
    "            'Numerical imputation (median) + Robust scaling',\n",
    "            'Categorical imputation (mode) + One-hot encoding',\n",
    "        ],\n",
    "        'numerical_features': len(numerical_features),\n",
    "        'categorical_features': len(categorical_features)\n",
    "    },\n",
    "    \n",
    "    'kaggle_submission': {\n",
    "        'file': submission_filename,\n",
    "        'path': submission_path,\n",
    "        'prediction_distribution': submission_df['Transported'].value_counts().to_dict(),\n",
    "    },\n",
    "    \n",
    "    'training_info': {\n",
    "        'models_tested': len(model_team),\n",
    "        'best_model': ultimate_champion_name,\n",
    "        'tuning_improvement': float(final_results[ultimate_champion_name]['improvement']),\n",
    "        'experiment_tracking': f\"mlflow ui --backend-store-uri {config.EXPERIMENT_DIR}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save model card\n",
    "card_path = os.path.join(model_save_dir, 'model_card.json')\n",
    "with open(card_path, 'w') as f:\n",
    "    json.dump(model_card, f, indent=2)\n",
    "print(f\"✅ Model card saved: {card_path}\")\n",
    "\n",
    "# 4. Save requirements\n",
    "requirements = {\n",
    "    'python': '3.8+',\n",
    "    'packages': {\n",
    "        'scikit-learn': '1.0+',\n",
    "        'pandas': '1.3+',\n",
    "        'numpy': '1.20+',\n",
    "        'mlflow': '2.0+',\n",
    "        'joblib': '1.0+',\n",
    "    }\n",
    "}\n",
    "\n",
    "req_path = os.path.join(model_save_dir, 'requirements.json')\n",
    "with open(req_path, 'w') as f:\n",
    "    json.dump(requirements, f, indent=2)\n",
    "print(f\"✅ Requirements saved: {req_path}\")\n",
    "\n",
    "print(f\"\\n🎉 ALL ARTIFACTS SAVED SUCCESSFULLY!\")\n",
    "print(\"\\n📚 YOUR SAVED WORK:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   • Trained Model: {model_path}\")\n",
    "print(f\"   • Preprocessing Pipeline: {pipeline_path}\")\n",
    "print(f\"   • Model Card: {card_path}\")\n",
    "print(f\"   • Kaggle Submission: {submission_path}\")\n",
    "print(f\"   • Experiment Tracking: mlflow ui --backend-store-uri {config.EXPERIMENT_DIR}\")\n",
    "\n",
    "print(f\"\\n🚀 MISSION ACCOMPLISHED! You're ready for Kaggle!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎨 Bonus: Simple Prediction Interface\n",
    "\n",
    "## 🔮 Let's Create a Fun Prediction Tool!\n",
    "\n",
    "This simple interface lets you make predictions for new passengers. It's like having your own spaceship transportation predictor!\n",
    "\n",
    "> 🎮 **Fun Exercise**: Try different passenger profiles and see if they'd be transported!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceshipPredictor:\n",
    "    \"\"\"\n",
    "    🚀 Simple interface for making passenger transportation predictions\n",
    "    \n",
    "    This class uses our trained model to predict whether new passengers\n",
    "    would be transported based on their characteristics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, pipeline):\n",
    "        self.model = model\n",
    "        self.pipeline = pipeline\n",
    "    \n",
    "    def predict_passenger(self, passenger_data):\n",
    "        \"\"\"Predict transportation for a single passenger\"\"\"\n",
    "        \n",
    "        # Convert to DataFrame with proper column names\n",
    "        passenger_df = pd.DataFrame([passenger_data])\n",
    "        \n",
    "        # Apply preprocessing pipeline\n",
    "        processed_data = self.pipeline.transform(passenger_df)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(processed_data)[0]\n",
    "        probability = self.model.predict_proba(processed_data)[0]\n",
    "        \n",
    "        return {\n",
    "            'transported': bool(prediction),\n",
    "            'probability_transported': probability[1],\n",
    "            'probability_not_transported': probability[0],\n",
    "            'confidence': max(probability)\n",
    "        }\n",
    "    \n",
    "    def predict_multiple(self, passengers_data):\n",
    "        \"\"\"Predict transportation for multiple passengers\"\"\"\n",
    "        passengers_df = pd.DataFrame(passengers_data)\n",
    "        processed_data = self.pipeline.transform(passengers_df)\n",
    "        \n",
    "        predictions = self.model.predict(processed_data)\n",
    "        probabilities = self.model.predict_proba(processed_data)\n",
    "        \n",
    "        results = []\n",
    "        for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "            results.append({\n",
    "                'passenger_id': i + 1,\n",
    "                'transported': bool(pred),\n",
    "                'probability_transported': prob[1],\n",
    "                'confidence': max(prob)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# 🚀 Initialize our predictor\n",
    "predictor = SpaceshipPredictor(final_champion, full_pipeline)\n",
    "\n",
    "print(\"🎨 SPACESHIP TITANIC PREDICTION INTERFACE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"🔮 Try our trained model with example passengers!\\n\")\n",
    "\n",
    "# Example passenger profiles\n",
    "example_passengers = [\n",
    "    {\n",
    "        'PassengerId': '0001_01',\n",
    "        'HomePlanet': 'Earth',\n",
    "        'CryoSleep': True,\n",
    "        'Cabin': 'B/123/P',\n",
    "        'Destination': 'TRAPPIST-1e',\n",
    "        'Age': 25,\n",
    "        'VIP': False,\n",
    "        'RoomService': 0,\n",
    "        'FoodCourt': 0,\n",
    "        'ShoppingMall': 0,\n",
    "        'Spa': 0,\n",
    "        'VRDeck': 0,\n",
    "        'Name': 'John Spacewalker'\n",
    "    },\n",
    "    {\n",
    "        'PassengerId': '0002_01',\n",
    "        'HomePlanet': 'Europa',\n",
    "        'CryoSleep': False,\n",
    "        'Cabin': 'A/001/S',\n",
    "        'Destination': '55 Cancri e',\n",
    "        'Age': 45,\n",
    "        'VIP': True,\n",
    "        'RoomService': 1000,\n",
    "        'FoodCourt': 500,\n",
    "        'ShoppingMall': 300,\n",
    "        'Spa': 800,\n",
    "        'VRDeck': 600,\n",
    "        'Name': 'Dr. Stella Stargazer'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make predictions for examples\n",
    "print(\"📊 EXAMPLE PREDICTIONS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, passenger in enumerate(example_passengers):\n",
    "    result = predictor.predict_passenger(passenger)\n",
    "    \n",
    "    print(f\"\\n👤 Passenger {i+1}: {passenger['Name']}\")\n",
    "    print(f\"   🌍 Home Planet: {passenger['HomePlanet']}\")\n",
    "    print(f\"   😴 CryoSleep: {passenger['CryoSleep']}\")\n",
    "    print(f\"   🎯 Destination: {passenger['Destination']}\")\n",
    "    print(f\"   👑 VIP: {passenger['VIP']}\")\n",
    "    \n",
    "    transport_status = \"🚀 TRANSPORTED\" if result['transported'] else \"❌ NOT TRANSPORTED\"\n",
    "    confidence_color = \"🟢\" if result['confidence'] > 0.8 else \"🟡\" if result['confidence'] > 0.6 else \"🔴\"\n",
    "    \n",
    "    print(f\"\\n   🔮 PREDICTION: {transport_status}\")\n",
    "    print(f\"   📊 Confidence: {confidence_color} {result['confidence']:.1%}\")\n",
    "    print(f\"   📈 Probability: {result['probability_transported']:.1%} transported, {result['probability_not_transported']:.1%} not transported\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🎉 PREDICTION INTERFACE READY!\")\n",
    "print(\"💡 You can use the predictor class in your own applications!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 Congratulations! Mission Accomplished! 🎉\n",
    "\n",
    "## 🌟 What You've Achieved:\n",
    "\n",
    "### 🏆 Your Accomplishments:\n",
    "1. **✅ Complete Data Investigation**: You explored and understood the Spaceship Titanic dataset\n",
    "2. **✅ Advanced Feature Engineering**: Created powerful new features from raw data\n",
    "3. **✅ Professional Preprocessing**: Built a robust pipeline that handles real-world data issues\n",
    "4. **✅ Comprehensive Model Training**: Tested multiple algorithms and selected the best one\n",
    "5. **✅ Hyperparameter Optimization**: Fine-tuned your model for peak performance\n",
    "6. **✅ Rigorous Evaluation**: Properly validated your model using train/val/test splits\n",
    "7. **✅ Kaggle-Ready Submission**: Created a properly formatted competition submission\n",
    "8. **✅ Production Artifacts**: Saved everything needed for deployment\n",
    "\n",
    "### 📊 Your Model's Performance:\n",
    f\"\"\"\n",
    "- **Test Accuracy**: {test_accuracy:.1%}\n",
    "- **Improvement over Baseline**: +{improvement_over_baseline:.1%}\n",
    "- **Final Model**: {ultimate_champion_name}\n",
    "- **Submission File**: {submission_filename}\n",
    \"\"\"\n",
    "\n",
    "## 🚀 Next Steps:\n",
    "\n",
    "1. **📤 Submit to Kaggle**: Upload your CSV file to the competition and see your score!\n",
    "2. **📊 Analyze Results**: Check the leaderboard and see how you compare\n",
    "3. **🔄 Iterate and Improve**: Try different features, models, or tuning strategies\n",
    "4. **🤝 Share Your Work**: Discuss your approach with the community\n",
    "\n",
    "### 💡 Pro Tips for Improvement:\n",
    "- **Feature Engineering**: Create even more creative features\n",
    "- **Model Ensembles**: Combine multiple models for better performance\n",
    "- **Advanced Tuning**: Use more sophisticated hyperparameter optimization\n",
    "- **Cross-Validation**: Try different validation strategies\n",
    "\n",
    "## 🎯 Key Lessons Learned:\n",
    "\n",
    "| Concept | Why It Matters |\n",
    "|---------|----------------|\n",
    "| **Data Exploration** | Understanding your data is the first step to good models |\n",
    "| **Feature Engineering** | Creative features can dramatically improve performance |\n",
    "| **Pipeline Construction** | Professional workflows prevent errors and save time |\n",
    "| **Model Evaluation** | Proper validation ensures your model actually works |\n",
    "| **Hyperparameter Tuning** | Small adjustments can lead to big improvements |\n",
    "| **Experiment Tracking** | MLflow helps you organize and reproduce your work |\n",
    "\n",
    "> 🌟 **You're now equipped with professional ML skills that apply to ANY classification problem!**\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 Final Mission Debrief:\n",
    "\n",
    "You started as a space detective investigating the Spaceship Titanic mystery. Through careful analysis, creative feature engineering, and rigorous model testing, you've built a powerful predictor that can determine which passengers were transported to another dimension!\n",
    "\n",
    "**Your mission is complete, but the journey continues!** 🚀\n",
    "\n",
    "### 📚 Resources to Continue Your Journey:\n",
    "- **Kaggle Competition**: [Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic)\n",
    "- **MLflow Documentation**: Learn more about experiment tracking\n",
    "- **Scikit-learn Guide**: Deep dive into machine learning algorithms\n",
    "- **Feature Engineering Books**: Master the art of creating great features\n",
    "\n",
    "---\n",
    "\n",
    "# 🎊 WELL DONE, DATA SCIENTIST! 🎊\n",
    "\n",
    "You've successfully completed your first interstellar machine learning mission! The skills you've learned here will serve you well in any data science project you undertake.\n",
    "\n",
    "**Now go forth and predict!** 🌌\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}