{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f34466",
   "metadata": {},
   "source": [
    "# 🏠 مشروع: التنبؤ بدرجة الصحة (Health Score Prediction) — بصيغة المحاضرة\n",
    "**وصف:** مشروع منظّم خطوة بخطوة لتنبؤ “Health Score” باستخدام نماذج انحدار: Decision Tree, Random Forest, XGBoost, LightGBM.\n",
    "\n",
    "\n",
    "**ملاحظات قبل التشغيل:**\n",
    "- كل خلية مستقلة ويمكن تشغيلها على حدة.\n",
    "- الشرح بالعربية مع كود قابل للتنفيذ في Python (Jupyter / Colab).\n",
    "- ستحصل على ملف `best_model.pkl` و`scaler.pkl` و`encoders.pkl` و`regression_models_results.csv` بعد تشغيل الخلايا النهائية.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d479085",
   "metadata": {},
   "source": [
    "## 🛠️ إعداد البيئة (Environment Setup)\n",
    "شرح: استيراد المكتبات الأساسية وضبط البيئة. شغّل هذه الخلية أولًا."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4846aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# مكتبات scikit-learn الأساسية\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# نماذج\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# مكتبات اختيارية (XGBoost, LightGBM)\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except Exception:\n",
    "    XGBRegressor = None\n",
    "    print('⚠️ xgboost غير مثبت — لتثبيت: pip install xgboost')\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "except Exception:\n",
    "    LGBMRegressor = None\n",
    "    print('⚠️ lightgbm غير مثبت — لتثبيت: pip install lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f937d9c2",
   "metadata": {},
   "source": [
    "## 📥 تحميل البيانات (Load Data)\n",
    "شرح: ضَع ملف البيانات `synthetic_health_data.csv` في نفس مجلد الـ Notebook، ثم شغّل الخلية لتحميل المعاينة."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81323dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"synthetic_health_data.csv\")\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"❌ لم أجد ملف البيانات: {DATA_PATH.resolve()}\\nأرفِع الملف أو حدّث مسار DATA_PATH.\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"✅ تم تحميل البيانات — الشكل:\", df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994dd2dc",
   "metadata": {},
   "source": [
    "## 🧠 تجهيز الميزات (Feature Engineering)\n",
    "شرح: إذا لم يكن عمود `Health_Score` موجودًا، سننشئه من مجموعة من الأعمدة الصحية باستخدام أوزان مبدئية قابلة للتعديل. بعدها نعرض وصفًا إحصائيًا للبيانات."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ac443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# إنشاء Health_Score إذا لم يكن موجودًا\n",
    "if \"Health_Score\" not in df.columns:\n",
    "    print(\"⚙️ إنشاء عمود Health_Score تلقائيًا باستخدام أوزان مبدئية...\")\n",
    "    # يمكنك تعديل هذه الأوزان بحسب خبرتك أو دراسة المجال\n",
    "    weights = {\n",
    "        \"Sleep_Hours\": 0.20,\n",
    "        \"Diet_Quality\": 0.25,\n",
    "        \"Activity_Level\": 0.25,\n",
    "        \"Stress_Level\": -0.15,\n",
    "        \"Blood_Pressure\": -0.10,\n",
    "        \"Age\": -0.05\n",
    "    }\n",
    "    # التحقق من وجود الأعمدة المطلوبة\n",
    "    missing = [c for c in weights.keys() if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"الأعمدة التالية مفقودة لإنشاء Health_Score: {missing}\")\n",
    "    df[\"Health_Score\"] = sum(df[col] * w for col, w in weights.items())\n",
    "    # إعادة تحجيم إلى نطاق 0-100\n",
    "    df[\"Health_Score\"] = 100 * (df[\"Health_Score\"] - df[\"Health_Score\"].min()) / (df[\"Health_Score\"].max() - df[\"Health_Score\"].min())\n",
    "else:\n",
    "    print(\"ℹ️ عمود Health_Score موجود بالفعل — سنستخدمه كما هو.\")\n",
    "\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722af71",
   "metadata": {},
   "source": [
    "## 📊 التحليل الاستكشافي (EDA)\n",
    "شرح: سنستعمل رسومات توضيحية لفهم العلاقات والتوزيع. (ملاحظة: نستخدم matplotlib لعرض الرسوم لضمان التوافق مع بيئات مختلفة)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e77058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# مصفوفة الارتباط (correlation matrix) ورسم خريطة حرارة باستخدام matplotlib\n",
    "corr = df.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.matshow(corr, cmap='coolwarm')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticks(range(len(corr.columns)))\n",
    "ax.set_yticks(range(len(corr.columns)))\n",
    "ax.set_xticklabels(corr.columns, rotation=90)\n",
    "ax.set_yticklabels(corr.columns)\n",
    "plt.title(\"خريطة الارتباط بين المتغيرات\", pad=20)\n",
    "plt.show()\n",
    "\n",
    "# توزيع الهدف\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.hist(df['Health_Score'].dropna(), bins=30)\n",
    "ax.set_title('توزيع Health_Score')\n",
    "ax.set_xlabel('Health_Score')\n",
    "ax.set_ylabel('العدد')\n",
    "plt.show()\n",
    "\n",
    "# رسم علاقة بين بعض المتغيرات المهمة و Health_Score\n",
    "pairs = [('Sleep_Hours', 'Health_Score'), ('Activity_Level', 'Health_Score'), ('Stress_Level', 'Health_Score')]\n",
    "for xcol, ycol in pairs:\n",
    "    if xcol in df.columns and ycol in df.columns:\n",
    "        fig, ax = plt.subplots(figsize=(6,4))\n",
    "        ax.scatter(df[xcol], df[ycol], alpha=0.6)\n",
    "        ax.set_xlabel(xcol)\n",
    "        ax.set_ylabel(ycol)\n",
    "        ax.set_title(f'{ycol} مقابل {xcol}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc45d6",
   "metadata": {},
   "source": [
    "## ✂️ الترميز (Encoding) والقياس (Scaling)\n",
    "شرح: ترميز الأعمدة النصية ثم قياس الميزات باستخدام StandardScaler. سنحفظ الـ encoders و scaler لاستخدامها لاحقًا عند النشر."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'Health_Score'\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype(float)\n",
    "\n",
    "encoders = {}\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        encoders[col] = le\n",
    "\n",
    "# ملء القيم الناقصة البسيط (إن وجدت) — هنا نعوض بالمتوسط للميزات العددية\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# حفظ مؤقت للـ encoders (لا تقم بالحفظ النهائي حتى نعرف أفضل نموذج)\n",
    "# joblib.dump(encoders, 'encoders_partial.pkl')\n",
    "print('✅ الترميز والقياس جاهزان. شكل X:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc6897",
   "metadata": {},
   "source": [
    "## 🔀 تقسيم البيانات (Train/Test Split)\n",
    "شرح: نقسم البيانات إلى مجموعة تدريب ومجموعة اختبار (80/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f542437",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print('✅ Training shape:', X_train.shape, 'Testing shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675e654",
   "metadata": {},
   "source": [
    "## ⚙️ تدريب النماذج وضبط المعاملات (Hyperparameter Tuning)\n",
    "شرح: سنستخدم `RandomizedSearchCV` لكل نموذج — الفراغات هنا أوسع قليلًا لتعكس منهج المحاضرة."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58880f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "def run_random_search(name, estimator, param_dist, n_iter=30, cv=3):\n",
    "    print(f\"\\n🔧 بدء الضبط: {name}\")\n",
    "    rs = RandomizedSearchCV(estimator, param_distributions=param_dist, n_iter=n_iter,\n",
    "                            scoring='r2', cv=cv, random_state=42, n_jobs=-1, verbose=0)\n",
    "    rs.fit(X_train, y_train)\n",
    "    best = rs.best_estimator_\n",
    "    y_pred = best.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"✅ {name} — R2_test: {r2:.4f} | MAE_test: {mae:.4f}\")\n",
    "    print('🔧 أفضل معاملات (best_params):', rs.best_params_)\n",
    "    return {'name': name, 'model': best, 'r2': r2, 'mae': mae, 'best_params': rs.best_params_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba310d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Decision Tree\n",
    "dt_params = {\n",
    "    'max_depth': [3, 5, 8, 12, None],\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 6),\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}\n",
    "results.append(run_random_search('DecisionTree', DecisionTreeRegressor(random_state=42), dt_params, n_iter=30))\n",
    "\n",
    "# Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [5, 8, 12, None],\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 6),\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "results.append(run_random_search('RandomForest', RandomForestRegressor(random_state=42), rf_params, n_iter=30))\n",
    "\n",
    "# XGBoost (if available)\n",
    "if XGBRegressor is not None:\n",
    "    xgb_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 8],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    results.append(run_random_search('XGBoost', XGBRegressor(eval_metric='rmse', random_state=42), xgb_params, n_iter=30))\n",
    "else:\n",
    "    print('⚠️ تخطي XGBoost — لم يتم العثور على المكتبة.')\n",
    "\n",
    "# LightGBM (if available)\n",
    "if LGBMRegressor is not None:\n",
    "    lgb_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'num_leaves': [15, 31, 63],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'min_data_in_leaf': [10, 20, 40]\n",
    "    }\n",
    "    results.append(run_random_search('LightGBM', LGBMRegressor(random_state=42), lgb_params, n_iter=30))\n",
    "else:\n",
    "    print('⚠️ تخطي LightGBM — لم يتم العثور على المكتبة.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb4ed6",
   "metadata": {},
   "source": [
    "## 📈 تقييم النماذج ومقارنة الأداء\n",
    "شرح: نلخّص النتائج في جدول ونرسم مقارنات R² و MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame([{\n",
    "    'Model': r['name'],\n",
    "    'R2_test': r['r2'],\n",
    "    'MAE_test': r['mae'],\n",
    "    'best_params': r['best_params']\n",
    "} for r in results])\n",
    "\n",
    "res_df = res_df.sort_values('R2_test', ascending=False).reset_index(drop=True)\n",
    "display(res_df[['Model','R2_test','MAE_test']])\n",
    "\n",
    "# رسم مقارنة R2\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.bar(res_df['Model'], res_df['R2_test'])\n",
    "ax.set_ylabel('R2 (Test)')\n",
    "ax.set_title('مقارنة النماذج حسب R2')\n",
    "plt.show()\n",
    "\n",
    "# رسم مقارنة MAE\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.bar(res_df['Model'], res_df['MAE_test'])\n",
    "ax.set_ylabel('MAE (Test)')\n",
    "ax.set_title('مقارنة النماذج حسب MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f9663",
   "metadata": {},
   "source": [
    "## 🏆 اختيار الأفضل وحفظه\n",
    "شرح: اختيار النموذج الأعلى R2 على الاختبار ثم حفظ النموذج والمحوّل والترميزات ونتائج الضبط."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32cef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = res_df.loc[res_df['R2_test'].idxmax()]\n",
    "best_name = best_row['Model']\n",
    "best_model_obj = next(r['model'] for r in results if r['name'] == best_name)\n",
    "\n",
    "print(f\"🏆 أفضل نموذج: {best_name} — R2_test = {best_row['R2_test']:.4f}\")\n",
    "\n",
    "joblib.dump(best_model_obj, 'best_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(encoders, 'encoders.pkl')\n",
    "res_df.to_csv('regression_models_results.csv', index=False)\n",
    "print('✅ حفظت: best_model.pkl, scaler.pkl, encoders.pkl, regression_models_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bdcc0",
   "metadata": {},
   "source": [
    "## 📝 الخلاصة (Summary & Insights)\n",
    "- لخص هنا النتائج الرئيسية: أي نموذج تفوق، قيم R² وMAE، وما المعاملات المهمة التي اكتشفناها.\n",
    "- اذكر توصيات عملية: مثل جمع بيانات إضافية، ميزات جديدة متوقعة التأثير، أو خطوات لتحسين الأداء (feature engineering, ensemble, stacking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1b183",
   "metadata": {},
   "source": [
    "## 🚀 تحضير للنشر (Deployment Preparation)\n",
    "شرح مختصر: يمكنك نشر `best_model.pkl` مع `scaler.pkl` و`encoders.pkl` عبر واجهة بسيطة باستخدام Streamlit أو Flask. فيما يلي مثال مبسط لكود Streamlit للنشر:\n",
    "\n",
    "```python\n",
    "# streamlit_app.py\n",
    "import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "model = joblib.load('best_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "encoders = joblib.load('encoders.pkl')\n",
    "\n",
    "st.title('Health Score Predictor')\n",
    "# ... إضافة واجهة مدخلات المستخدم ثم التنبؤ\n",
    "```\n",
    "\n",
    "— انتهى المشروع.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
